<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>C++复现huggingface transformers中的beam search | 自拙集</title><meta name="keywords" content="LLM,huggingface,inference"><meta name="author" content="Richard YU"><meta name="copyright" content="Richard YU"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="description" content="由于平时工作内容是LLM 推理加速，所以需要使用c++来实现并适配decoding阶段的beam search方法。下面将以intel neural-speed v1.0的代码为例进行讲解和记录。  neural_speed以早期的llama.cpp为基础构建而来，增加了intel平台的kernel优化以及一些新的推理优化技术（比如streaming-llm, continuous batchin"><meta property="og:type" content="article"><meta property="og:title" content="C++复现huggingface transformers中的beam search"><meta property="og:url" content="http://densecollections.top/posts/c++_beam-search-in-neural-speed/index.html"><meta property="og:site_name" content="自拙集"><meta property="og:description" content="由于平时工作内容是LLM 推理加速，所以需要使用c++来实现并适配decoding阶段的beam search方法。下面将以intel neural-speed v1.0的代码为例进行讲解和记录。  neural_speed以早期的llama.cpp为基础构建而来，增加了intel平台的kernel优化以及一些新的推理优化技术（比如streaming-llm, continuous batchin"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://plus.unsplash.com/premium_photo-1683121246444-0851419273d8?q=80&w=1780&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"><meta property="article:published_time" content="2024-09-10T03:58:56.000Z"><meta property="article:modified_time" content="2024-09-10T06:21:35.535Z"><meta property="article:author" content="Richard YU"><meta property="article:tag" content="LLM"><meta property="article:tag" content="huggingface"><meta property="article:tag" content="inference"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://plus.unsplash.com/premium_photo-1683121246444-0851419273d8?q=80&w=1780&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://densecollections.top/posts/c++_beam-search-in-neural-speed/"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="preconnect" href="//zz.bdstatic.com"><link rel="manifest" href="/image/pwa/manifest.json"><link rel="apple-touch-icon" sizes="180x180" href="/image/pwa/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/image/pwa/32.png"><link rel="icon" type="image/png" sizes="16x16" href="/image/pwa/16.png"><link rel="mask-icon" href="/image/pwa/safari-pinned-tab.svg" color="#5bbad5"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload='this.media="all"'><script>var GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"search.xml",languages:{hits_empty:"找不到您查询的内容：${query}"}},translate:void 0,noticeOutdate:void 0,highlight:{plugin:"highlighjs",highlightCopy:!0,highlightLang:!0},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!1,post:!1},runtime:"",date_suffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:{limitCount:50,languages:{author:"作者: Richard YU",link:"链接: ",source:"来源: 自拙集",info:"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},lightbox:"fancybox",Snackbar:void 0,source:{jQuery:"https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js",justifiedGallery:{js:"https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js",css:"https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css"},fancybox:{js:"https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js",css:"https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"}},isPhotoFigcaption:!0,islazyload:!1,isanchor:!1},saveToLocal={set:function(e,t,s){const n=864e5*s,a={value:t,expiry:(new Date).getTime()+n};localStorage.setItem(e,JSON.stringify(a))},get:function(e){const t=localStorage.getItem(e);if(!t)return;const s=JSON.parse(t);if(!((new Date).getTime()>s.expiry))return s.value;localStorage.removeItem(e)}};const getScript=e=>new Promise((t,s)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=s,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,t())},document.head.appendChild(n)})</script><script id="config_change">var GLOBAL_CONFIG_SITE={isPost:!0,isHome:!1,isHighlightShrink:!0,isToc:!0,postUpdate:"2024-09-10 14:21:35"}</script><noscript><style type="text/css">#nav{opacity:1}.justified-gallery img{opacity:1}#post-meta time,#recent-posts time{display:inline!important}</style></noscript><script>!function(){window.activateDarkMode=function(){document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},window.activateLightMode=function(){document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const e=saveToLocal.get("theme");"dark"===e?activateDarkMode():"light"===e&&activateLightMode();const t=saveToLocal.get("aside-status");void 0!==t&&("hide"===t?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"))}()</script><style type="text/css">.app-refresh{position:fixed;top:-2.2rem;left:0;right:0;z-index:99999;padding:0 1rem;font-size:15px;height:2.2rem;transition:all .3s ease}.app-refresh-wrap{display:flex;color:#fff;height:100%;align-items:center;justify-content:center}.app-refresh-wrap a{color:#fff;text-decoration:underline;cursor:pointer}</style><meta name="generator" content="Hexo 5.4.2"><link rel="alternate" href="/atom.xml" title="自拙集" type="application/atom+xml">
<link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/others/avatar.jpg" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">32</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">62</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">15</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> AboutMe</span></a></div><div class="menus_item"><a class="site-page" href="/MindWandering/"><i class="fa-fw fas fa-paper-plane"></i><span> MindWandering</span></a></div><div class="menus_item"><a class="site-page" href="/FunStuff/"><i class="fa-fw fa-solid fa-eye"></i><span> FunStuff</span></a></div><div class="menus_item"><a class="site-page" href="/PaperStation/"><i class="fa-fw fas fa-edit"></i><span> PaperStation</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archive</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div></div></div></div><div id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url(https://plus.unsplash.com/premium_photo-1683121246444-0851419273d8?q=80&amp;w=1780&amp;auto=format&amp;fit=crop&amp;ixlib=rb-4.0.3&amp;ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">自拙集</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> AboutMe</span></a></div><div class="menus_item"><a class="site-page" href="/MindWandering/"><i class="fa-fw fas fa-paper-plane"></i><span> MindWandering</span></a></div><div class="menus_item"><a class="site-page" href="/FunStuff/"><i class="fa-fw fa-solid fa-eye"></i><span> FunStuff</span></a></div><div class="menus_item"><a class="site-page" href="/PaperStation/"><i class="fa-fw fas fa-edit"></i><span> PaperStation</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archive</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">C++复现huggingface transformers中的beam search</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-09-10T03:58:56.000Z" title="发表于 2024-09-10 11:58:56">2024-09-10</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-09-10T06:21:35.535Z" title="更新于 2024-09-10 14:21:35">2024-09-10</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/LLM%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6/">LLM推理框架</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">5.2k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>18分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>由于平时工作内容是LLM 推理加速，所以需要使用c++来实现并适配decoding阶段的beam search方法。下面将以intel neural-speed <a target="_blank" rel="noopener" href="https://github.com/intel/neural-speed/blob/v1.0/neural_speed/models/model_utils/model_utils.cpp#L2771">v1.0</a>的代码为例进行讲解和记录。</p><blockquote><p><code>neural_speed</code>以早期的<a target="_blank" rel="noopener" href="https://github.com/ggerganov/llama.cpp">llama.cpp</a>为基础构建而来，增加了intel平台的<a target="_blank" rel="noopener" href="https://github.com/intel/neural-speed/tree/v1.0/bestla">kernel优化</a>以及一些新的推理优化技术（比如streaming-llm, continuous batching）等，在server和client的机器上都有一些不错的效果。</p></blockquote><span id="more"></span><h2 id="beam-search的基本思想">beam search的基本思想</h2><p>参考博客：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/114669778">十分钟读懂beam search</a>, <a target="_blank" rel="noopener" href="https://www.cnblogs.com/xiximayou/p/17353352.html">LLM在解码时如何生成文本</a></p><p>语言模型在生成下一个token时，理论做法是根据前面的token来一起做最大似然估计。从实现上来说，如果要生成长度为<code>L</code>的新文本，假设模型的<code>n_vocab</code>是<code>N</code>,就要比较<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>N</mi><mi>L</mi></msup></mrow><annotation encoding="application/x-tex">N^{L}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8413309999999999em;vertical-align:0"></span><span class="mord"><span class="mord mathdefault" style="margin-right:.10903em">N</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8413309999999999em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">L</span></span></span></span></span></span></span></span></span></span></span></span>次组合，选择连乘概率最大的，这么多次模型推理次数肯定是不现实的。另一种token generation方法<code>Greedy Search</code>是使用局部最优的方式，每次选择推理结束时logits最大的那个token id，虽然速度快，但是句子的质量一般不会太好。</p><p>beam search是介于两者之间的，通过设置beam size来做速度和质量的trade off.简单来说就是每次只选beam size个组合，生成句子的内容可能在下一个步骤因为其分数值过低而被替换，但是总体依旧保持在固定数量的备选组合中。</p><p>下面这个图简单示意了beam search (beam size =2 ) 的过程，不过实际情况中可能<code>ABD</code>可能会被(假设）<code>CEC</code>取代，不一定会是一直保持<code>A</code>开头。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/c++_beam-search_in_neural-speed/beam_search_1.png" alt></p><h2 id="transformers的实现">transformers的实现</h2><p>为了快速并且规范的了解beam search的工程实现，我们以huggingface transformers v4.38.2为例.</p><p><a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/blob/v4.38.2/src/transformers/generation/utils.py#L2821">beam search接口</a></p><p><a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/blob/v4.38.2/src/transformers/generation/beam_search.py#L123">beam search scorer</a></p><p><a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/blob/v4.38.2/src/transformers/generation/beam_search.py#L922">beam hypotheses</a></p><p>主函数前面都是一些准备和校验工作，我们直接从logits获得那部分开始看。</p><p>首先是对logits进行处理，<a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/blob/v4.38.2/src/transformers/generation/utils.py#L3079-L3087">code</a>. logits→logsoftmax→logits_processor→beam scores. 其中logits_process是针对一些用户配置的generation_config做的处理， 我们这里以<code>min_new_tokens</code>为例，logits_processor会dispatch到min_new_token_logits_processor,对未达到min_new_tokens长度的beam的当前新logits的eos_token_id位置的logit值置为负无穷，强制让其选不到eos token,即使该token的概率值可能是最大的。 beam scores部分就是将当前token logits 分数累加该beam的之前的分数。logits的shape为[bs * beam_size, 1, n_vocab], beam scores的shape是[bs * beam_size, 1],相加会做broadcast操作。</p><p>接下来(<a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/blob/v4.38.2/src/transformers/generation/utils.py#L3108-L3119">code</a>)，选取top_k个next tokens,并且计算出token id以及beam indices.其中把next_token_scores reshape成[bs, beam_size * vocab]这一步是为了在每个batch的所有beam上选top-k, 而不是一个个beam选最后再放一起合并，但实际上我是对其reshape成[bs * beam_size, vocab]来做的，因为我是考虑到每个batch的beam_szie可能不同，比如server服务或者一开始进行beam_search之前的prompt的num_beam实际是1，后面才是beam_size个，另一方面把omp对第一维的for并行增大一些。还有一个要注意的是为什么top_k的k是大于等于2 * beam_size，为什么不直接是beam_size，这样不是刚好就可以足够更新了吗？这是因为transformers实现里面还有一个单独的容器去存储提前达到eos_token的beam,这样的话剩下的几个beam 数量就小于beam_size个，缺少的beam我们会继续从top_k中的池子去继续补充，因为有可能补充进来的beam最后的score不一定会比提前到达eos_token的beam差,所以选2倍的话就是如果每个beam都恰好到了eos_token,那么还剩下beam_size个可以补充（每个beam在vocab中选两个，所以不可能剩下的beam_size个beam还有的会到eos_token，所以一定可以打满）。最后得到之后即使确定top_k个next_token分别是什么，以及它属于哪个beam_indices.</p><p>然后通过<code>BeamScorer</code>这个类来接受上面的信息更新beam.我们先看下这个类都有哪些成员<a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/blob/v4.38.2/src/transformers/generation/beam_search.py#L162-L210">code</a>. 其中length_penalty和do_early_stopping都是generation_config中的两个配置，前者是对beam的tokens长短进行进一步的打分，后者影响最后beam输出的时机和比较的策略，这两个我们后面会再讲。里面group的概念是对beam_size这个维度的处理再做一个切分，暂时不知道是什么用处，我在C++实现里也暂时忽略了这个变量，我们就把它直接当成是1处理。我们重点看self._beam_hyps和self._done这两个变量，前者就是装到了eos_token的beam的容器BeamHypotheses集合，大小是batch_size个，每个BeamHypotheses的大小是beam_size个，后者代表所有batch_size的beam search都结束了才能结束整个流程，所以是<code>self._done.all()</code>,每个batch是否结束会看BeamHypotheses，这个类也有一个<a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/blob/v4.38.2/src/transformers/generation/beam_search.py#L971-L1005">is_done</a>的函数,这个函数是由<code>early_stopping</code>来进行控制的，写的比较绕，我们一步步来看。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self._done[batch_group_idx] = self._done[batch_group_idx] <span class="keyword">or</span> self._beam_hyps[batch_group_idx].is_done(</span><br><span class="line">                next_scores[batch_idx].<span class="built_in">max</span>().item(), cur_len, decoder_prompt_len</span><br><span class="line">            )</span><br><span class="line">            </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">is_done</span>(<span class="params">self, best_sum_logprobs: <span class="built_in">float</span>, cur_len: <span class="built_in">int</span>, decoder_prompt_len: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="number">0</span></span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        If there are enough hypotheses and that none of the hypotheses being generated can become better than the worst</span></span><br><span class="line"><span class="string">        one in the heap, then we are done with this sentence.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(self) &lt; self.num_beams:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># `True`: stop as soon as at least `num_beams` hypotheses are finished</span></span><br><span class="line">        <span class="keyword">if</span> self.early_stopping <span class="keyword">is</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="comment"># `False`: heuristic -- compute best possible score from `cur_len`, even though it is not entirely accurate</span></span><br><span class="line">        <span class="comment">#  when `length_penalty` is positive. See the discussion below for more details.</span></span><br><span class="line">        <span class="comment"># https://github.com/huggingface/transformers/pull/20901#issuecomment-1369845565</span></span><br><span class="line">        <span class="keyword">elif</span> self.early_stopping <span class="keyword">is</span> <span class="literal">False</span>:</span><br><span class="line">            highest_attainable_score = best_sum_logprobs / (cur_len - decoder_prompt_len) ** self.length_penalty</span><br><span class="line">            ret = self.worst_score &gt;= highest_attainable_score</span><br><span class="line">            <span class="keyword">return</span> ret</span><br><span class="line">        <span class="comment"># `&quot;never&quot;`: compute the best possible score, depending on the signal of `length_penalty`</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># `length_penalty` &gt; 0.0 -&gt; max denominator is obtaned from `max_length`, not from `cur_len` -&gt; min</span></span><br><span class="line">            <span class="comment"># abs(`highest_attainable_score`) is obtained -&gt; `highest_attainable_score` is negative, hence we obtain</span></span><br><span class="line">            <span class="comment"># its max this way</span></span><br><span class="line">            <span class="keyword">if</span> self.length_penalty &gt; <span class="number">0.0</span>:</span><br><span class="line">                <span class="keyword">if</span> self.max_length &lt;= decoder_prompt_len:</span><br><span class="line">                    <span class="keyword">raise</span> ValueError(<span class="string">&quot;max_length is not larger than decoder prompt length&quot;</span>)</span><br><span class="line">                highest_attainable_score = (</span><br><span class="line">                    best_sum_logprobs / (self.max_length - decoder_prompt_len) ** self.length_penalty</span><br><span class="line">                )</span><br><span class="line">            <span class="comment"># the opposite logic applies here (max `highest_attainable_score` from `cur_len`)</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                highest_attainable_score = best_sum_logprobs / (cur_len - decoder_prompt_len) ** self.length_penalty</span><br><span class="line">            ret = self.worst_score &gt;= highest_attainable_score</span><br><span class="line">            <span class="keyword">return</span> ret</span><br></pre></td></tr></table></figure><p>1). 容量是beam_size个，没塞满，返回false,容易理解，不多赘述</p><p>2). 塞满了beam_size个，但是early_stopping值是true，那么返回true，意思是说我也不会去比较还没塞进去的剩下的那些没到eos_token的beam（top_k那里有解释），直接返回这个hypotheses里面的最大score的为最终的generated tokens.</p><p>3). 塞满了beam_size个，但是<code>early_stopping</code>值是false,这个时候是比较这个<code>BeamHypotheses</code>里面最小score是不是比当前这个batch的所有beam的最大score都大，是的话返回true，表示后面再继续下去也不会更大了,否则就返回false，继续扩展beam.这里的疑问是，为什么是选择最小的去比所有beam里面最大的，而不是最大比最大？第一是beam的最终分数，是加入<code>BeamHypotheses</code>时候才最终确定的，除了累加<code>logsoftmax</code>之外，还要除以<code>gen_len ** length_penalty</code>，<code>gen_len</code>是新生成的new token的长度，<code>length_penalty</code>可正可负，默认值是1，为正的时候分母越来越大，由于<code>logsoftmax</code>是负数，累加越来越小，所以最终score是变大的，所以是偏好长的结果，为负的时候分母越来越大，所以最终score变小，所以是偏好短的句子。第二，和所有beam的最大score比的时候，这个score也要除以<code>gen_len ** length_penalty</code>来确定最后分数，假如<code>length_penalty</code>为负数，那么以后再继续扩展beam也不会比当前大（注意，继续扩展每个beam都有同样的<code>gen_len</code>)，所以肯定返回true是没问题的，甚至我觉得拿最大比最大也没啥逻辑问题（当然这里可能还有一个原因就是返回的不一定是top1的beam，可能返回几个备选结果出来）。如果<code>length_penalty</code>为正，这个时候你不能保证继续拓展beam不会让score变大，毕竟<code>gen_len</code>带动分母变大,所以这个我觉得你拿最大或者最小比所有当前beam的最大也不好说。但是代码的注释<a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/pull/20901#issuecomment-1369845565">解释</a>了一下这个是启发式的，对结果影响比较小，同时也能减少迭代次数（加速）。</p><p>4). 塞满了beam_size个，但是<code>early_stopping</code>是<code>never</code> （对，你没看错，在transformers里，<code>early_stopping</code>是三元的，参见上面的注释解释部分）。这个是为了mathematically correct,毕竟beam search理论中不存在什么<code>early_stopping</code>，这个开关只是为了工程近似和加速。这里就是根据<code>length_penalty</code>的正负和新生成的长度来精确确定最大score.但是实际上我也没怎么看到有人用这种方式。</p><p>综上，<code>BeamHypotheses</code>是存放最后要选择出最好的beam的池子，里面的beam可能是带了eos_token，也可能没有，这个由early_stopping确定。<code>BeamScorer</code>的process过程就是往里面加hypo，finalize的过程就是整理池子并选出来，<code>is_done()</code>的作用就是告诉finalize需不需要整理池子（主要是节省next token计算量）。我在后面会接着讲这两个过程。</p><p>我自己在C++实现时候对<code>is_done</code>的逻辑做了简化，我只给<code>early_stopping</code>设置true或false两个选项，true就是为了提前结束（eos_token), false就是数学上一个个比较选出最大的。因为我们的工作主要关注在server CPU端，bs不会像GPU那样打得很大，所以计算量上相比于transformers的逻辑不会差太多，而且还容易理解。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> batch_idx <span class="keyword">in</span> <span class="built_in">range</span>(batch_size):</span><br><span class="line">           batch_group_idx = batch_idx * self.num_beam_groups + group_index</span><br><span class="line">           <span class="keyword">if</span> self._done[batch_group_idx]:</span><br><span class="line">               <span class="keyword">if</span> self.num_beams &lt; <span class="built_in">len</span>(self._beam_hyps[batch_group_idx]):</span><br><span class="line">                   <span class="keyword">raise</span> ValueError(<span class="string">f&quot;Batch can only be done if at least <span class="subst">&#123;self.num_beams&#125;</span> beams have been generated&quot;</span>)</span><br><span class="line">               <span class="keyword">if</span> eos_token_id <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> pad_token_id <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                   <span class="keyword">raise</span> ValueError(<span class="string">&quot;Generated beams &gt;= num_beams -&gt; eos_token_id and pad_token have to be defined&quot;</span>)</span><br><span class="line">               <span class="comment"># pad the batch</span></span><br><span class="line">               next_beam_scores[batch_idx, :] = <span class="number">0</span></span><br><span class="line">               next_beam_tokens[batch_idx, :] = pad_token_id</span><br><span class="line">               next_beam_indices[batch_idx, :] = <span class="number">0</span></span><br><span class="line">               <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">           <span class="comment"># next tokens for this sentence</span></span><br><span class="line">           beam_idx = <span class="number">0</span></span><br><span class="line">           <span class="keyword">for</span> beam_token_rank, (next_token, next_score, next_index) <span class="keyword">in</span> <span class="built_in">enumerate</span>(</span><br><span class="line">               <span class="built_in">zip</span>(next_tokens[batch_idx], next_scores[batch_idx], next_indices[batch_idx])</span><br><span class="line">           ):</span><br><span class="line">               batch_beam_idx = batch_idx * self.group_size + next_index</span><br><span class="line">               <span class="comment"># add to generated hypotheses if end of sentence</span></span><br><span class="line">               <span class="keyword">if</span> (eos_token_id <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>) <span class="keyword">and</span> (next_token.item() <span class="keyword">in</span> eos_token_id):</span><br><span class="line">                   <span class="comment"># if beam_token does not belong to top num_beams tokens, it should not be added</span></span><br><span class="line">                   is_beam_token_worse_than_top_num_beams = beam_token_rank &gt;= self.group_size</span><br><span class="line">                   <span class="keyword">if</span> is_beam_token_worse_than_top_num_beams:</span><br><span class="line">                       <span class="keyword">continue</span></span><br><span class="line">                   <span class="keyword">if</span> beam_indices <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                       beam_index = beam_indices[batch_beam_idx]</span><br><span class="line">                       beam_index = beam_index + (batch_beam_idx,)</span><br><span class="line">                   <span class="keyword">else</span>:</span><br><span class="line">                       beam_index = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">                   self._beam_hyps[batch_group_idx].add(</span><br><span class="line">                       input_ids[batch_beam_idx].clone(),</span><br><span class="line">                       next_score.item(),</span><br><span class="line">                       beam_indices=beam_index,</span><br><span class="line">                       generated_len=cur_len - decoder_prompt_len,</span><br><span class="line">                   )</span><br><span class="line">               <span class="keyword">else</span>:</span><br><span class="line">                   <span class="comment"># add next predicted token since it is not eos_token</span></span><br><span class="line">                   next_beam_scores[batch_idx, beam_idx] = next_score</span><br><span class="line">                   next_beam_tokens[batch_idx, beam_idx] = next_token</span><br><span class="line">                   next_beam_indices[batch_idx, beam_idx] = batch_beam_idx</span><br><span class="line">                   beam_idx += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">               <span class="comment"># once the beam for next step is full, don&#x27;t add more tokens to it.</span></span><br><span class="line">               <span class="keyword">if</span> beam_idx == self.group_size:</span><br><span class="line">                   <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">           <span class="keyword">if</span> beam_idx &lt; self.group_size:</span><br><span class="line">               <span class="keyword">raise</span> ValueError(</span><br><span class="line">                   <span class="string">f&quot;At most <span class="subst">&#123;self.group_size&#125;</span> tokens in <span class="subst">&#123;next_tokens[batch_idx]&#125;</span> can be equal to `eos_token_id:&quot;</span></span><br><span class="line">                   <span class="string">f&quot; <span class="subst">&#123;eos_token_id&#125;</span>`. Make sure <span class="subst">&#123;next_tokens[batch_idx]&#125;</span> are corrected.&quot;</span></span><br><span class="line">               )</span><br><span class="line"></span><br><span class="line">           <span class="comment"># Check if we are done so that we can save a pad step if all(done)</span></span><br><span class="line">           self._done[batch_group_idx] = self._done[batch_group_idx] <span class="keyword">or</span> self._beam_hyps[batch_group_idx].is_done(</span><br><span class="line">               next_scores[batch_idx].<span class="built_in">max</span>().item(), cur_len, decoder_prompt_len</span><br><span class="line">           )</span><br><span class="line"></span><br><span class="line">       <span class="keyword">return</span> UserDict(</span><br><span class="line">           &#123;</span><br><span class="line">               <span class="string">&quot;next_beam_scores&quot;</span>: next_beam_scores.view(-<span class="number">1</span>),</span><br><span class="line">               <span class="string">&quot;next_beam_tokens&quot;</span>: next_beam_tokens.view(-<span class="number">1</span>),</span><br><span class="line">               <span class="string">&quot;next_beam_indices&quot;</span>: next_beam_indices.view(-<span class="number">1</span>),</span><br><span class="line">           &#125;</span><br><span class="line">       )</span><br></pre></td></tr></table></figure><p>兜了一圈后，我们现在具体来看<code>BeamScorer</code>这个类如何更新beam，也就是process函数做了哪些事。我们跳过变量准备部分，直接看两个大for，第一个for是batch维度分开处理，第二个for是对选出的next tokens处理（top_k采出了2* beam_size个）。第一个for里面首先对已经done的batch （done不done由它对应的<code>BeamHypotheses</code>决定）做了padding处理，这样就是防止index乱了，实际上我觉得可以做batch reductio，只不过会带来管理的难度，可以理解transformers为啥不这么做。接下来开始正式更新beam，next token等变量带来的信息有：token_id,累加的logsoftmax值，已经这个token是属于哪个beam分支（beam_index）。如果next_token是eos_token, 按照之前说的，这个beam的token generate完了，应该add到<code>self._beam_hyps</code>中，否则就作为下一个token进行推理。但是这里做了一个判断就是如果这个next_eos_token不是top_beam_size里面的，就不应该被放进<code>self._beam_hyps</code>（数学上好像没有合理的解释？），具体的原因我觉得是beam search理论上就是每步都维持在top_beam_size个，而<code>next_beam_scores</code>永远放满beam_size个应该是为了各种固定的shape考量，维持其不变。最后更新完beams之后检查下这个batch是否done.</p><p>到现在这一步，我们完成了从获取模型推理的logits到打分并选择top_k next token，到更新beams状态，接下来，模型会拿到next_token进行推理，然后判断所有的batch是否都结束了（is_done)或者到了指定的最大生成长度，未结束就继续重复前面的步骤。</p><blockquote><p>在推理前其实模型还会更新一下kv cache, 这里如何更新的我并没有仔细研究，看样子是交给了不同的模型去实现自己的版本，本质上要做的事就是根据你选出的非eos_token对应的上一个的beam_index来重排一下kv cache，比如说上次的beam indices是[0,1,2,3],选出的next_token对应的beam_index是[2,1,1,0] （假设排序顺序是按照beam_score降序），那么新的kv_cache更新伪代码则是<code>kv_cache_new[i] = kv_cache[j] for i in [0,1,2,3], j in [2,1,1,0]</code>.当然重新开辟一份内存比较低效，实际中我估计可能是swap操作（具体实现不得而知），我在c++的实现中是使用原地memcpy操作（按照顺序），如果框架使用了paged_attention或者indirect kv cache，可能swap操作会比较方便。</p></blockquote><blockquote><p>这里有一个小问题是刚送进beam_search函数的prompt是如何推理的？因为不足beam_size个，为了兼容while，要么是扩张input到beam_size个，要么是扩张logits到beam_size个，但是这两种在while的top_k里都会选出重复的token, 而非正确的。看注释送进来的inputs就已经是beam_size个了，所以也不知道是怎么处理的。我在C++实现里是把这部分与while True的next_token分开处理的。如果有小伙伴知道transformers是如何处理first token的，欢迎留言告诉我~</p></blockquote><p>最后如果打破循环之后，使用<code>BeamScorer</code>来进行finalize, <a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/blob/v4.38.2/src/transformers/generation/beam_search.py#L335-L374">code</a>. 逻辑也比较简单，就是for batch去检查beam_hyps，done的话就continue，没有的话就把剩余的beam往里面add（beam_hyps自己根据score维持到beam_size个beam).然后选择每个batch对应的beam_hyps的最大的beam作为结果输出。这里面还有一些代码是为了找出每个batch的最大length，然后padding不足这个最大length的剩余部分，以维持一些兼容性。</p><h2 id="c-实现">c++实现</h2><p>根据上面对<code>transformers</code> beam_search的代码分析，并对一些逻辑进行简化（比如early_stopping判断is_done), 大致确定了复现的框架，如下图。</p><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/c++_beam-search_in_neural-speed/beam_search_2.png" alt></p><ul><li><p>对输入的prompt进行推理，拿到top beam_size个next_token，组成current_beams</p></li><li><p>拷贝kv_cache beam index 0到其他beam，准备好kv_cache</p></li><li><p>从current_beams拿到模型输入，inference 一次</p></li><li><p>拿到原始logits,对其进行处理，这里只有一个min_new_tokens 对应的logits_processor</p></li><li><p>打分：对处理后的logits进行logsoftmax处理，并累加对应beam的score</p></li><li><p>从打分后的logits选出top_k个 （2*beam_size）</p></li><li><p>根据top_k的next_token等信息，进行筛选，是eos_token并且在top_beam_size内就add进beam_hypothesis中，否则组成next_beams</p></li><li><p>根据cur_beams和next_beams的beam_indices来进行kv cache update,为下一次model的inference做准备</p></li><li><p>检查是否is_done, 条件是根据beam_hypothesis是否满（do early_stopping)或到了max_new_tokens</p></li><li><p>非done，swap next_beams和current_beams内存，继续从第一步开始</p></li><li><p>是done，根据beam_hypothesis是否done来决定加不加入剩余的beams，最终返回其中的top_1 beam最为输出。</p></li></ul><p>除此之外，我们在整个beam search有关的流程中加了一些打印，方便在debug截断看到每一步的结果，也容易来和transformers库的结果比较 （FP32 data type）。</p><p>1). 先看一下<code>beam_next_token</code>, <code>beam</code>和<code>beam_hypotheses</code>的定义，这三者是依次“包含”的关系，<a target="_blank" rel="noopener" href="https://github.com/intel/neural-speed/blob/v1.0/neural_speed/models/model_utils/model_utils.h#L287-L396">code</a>. 这三个struct没什么好说的，其中<code>beam</code>中的<code>request_idx</code>代表着输入的prompt的id,是狭义上的batch_size维度，这个是为了对应多batch推理而加的，也会和后面的batch reduction以及未来要说的continuous batching有关。<code>beam_hypotheses</code>中的<code>is_done</code>函数如前所述，对<code>early_stopping</code>为False和never做了简化。</p><p>2). 对logits的处理：<code>logits_processor</code>(<a target="_blank" rel="noopener" href="https://github.com/intel/neural-speed/blob/v1.0/neural_speed/models/model_utils/model_utils.cpp#L2044-L2068">code</a>),对不到<code>min_new_tokens</code>的beams进行eos_token mask 负无穷处理，代码比较直接；<code>logits_info</code>(<a target="_blank" rel="noopener" href="https://github.com/intel/neural-speed/blob/v1.0/neural_speed/models/model_utils/model_utils.cpp#L1968-L2042">code</a>)，计算log_softmax，选择top_k个next_token. 这两者都加了batch维度的处理，所以要注意一下<code>bs_stride</code>以及只选最后一个token的vector的offset的计算。在<code>logits_info</code>中，softmax的计算参考了比较常见的数值稳定的实现，即减去最大值再做exp，exp也是调用了标准库而非近似计算，为了保证精度（近似计算的实验未做,具体能差多少，能带来多少速度提升暂时不知道），但并未对所有vocab_size个数值进行exp+log（分子上），仅对需要的top_k个做，这样可以降低一些计算量，此外还针对batch_size维度做了并行（实际上可以在vocab维度上分段并行来寻求max_element值，虽有提升但对整体影响较小，这里为了简洁未采用）。top_k next_token通过最小堆来实现（vector+std::make_heap)，对每个batch维度返回一个大小为top_k的最小堆。</p><blockquote><p>logits的处理上，batch的大小并不是简单等于prompt batch_size，也不一定等于prompt_batch_size * beam_size，而是running_prompt_batch_size * beam_size,因为有batch reduction （会有提前结束的某个请求）。</p></blockquote><p>3). 对kv_cache的处理。Neural_Speed的kv_cache是预先申请好的一块足够的buffer，在beam_search中的update主要通过类<a target="_blank" rel="noopener" href="https://github.com/intel/neural-speed/blob/v1.0/neural_speed/models/model_utils/model_utils.h#L419">beam_search_kv_cache_reorder</a>来实现，其更新的主要代码在<a target="_blank" rel="noopener" href="https://github.com/intel/neural-speed/blob/v1.0/neural_speed/models/model_utils/model_utils.cpp#L2070-L2131">这里</a>。我们首先考虑一下kv_cache update需要哪些信息和步骤：</p><ul><li><p>在首次推理时，每个prompt我们是只推一次而非repeat prompt到beam_size个，也就是说只会填充beam_index 为0的一个kv cache块，其他块是空，所以我们要对刚推完prompt的kv_cache进行repeat （prefill之后memcpy kv_cache，非paged_attention）</p></li><li><p>在其后的generation阶段，会根据新的score来更新beams （cur_beams→next_beams)，假设beam_size为4，那么推理之前的beam_indices是<code>[0,1,2,3]</code>，推理之后的beam_indices假设变成了<code>[0,1,1,2]</code>,即next_beams现在把原先的cur_beams的indices=2和3的位置的beam丢弃了，换成了cur_beams的indices=1和2的位置的beam (加新的next_token)。那么，我们要把kv_cache中indices=1的值拷贝到indices=2的位置，把原来indices=2的值拷贝到indices=3的位置。为了完成这样的拷贝，第一我们要知道推理之后的indices,第二，我们按照顺序拷贝，否则会丢失原来的数据，比如上面的例子，应该先拷贝2到3，然后才能拷贝1到2,将这两个信息合在一起，就是给出一个带顺序的拷贝的src和dst idx的容器即可。</p></li><li><p>每个模型的kv_cache存储的layout不一定一样，简单点说，就是shape可能不一样，这对memcpy造成了困难，一种方法就是每种模型实现一套自己的拷贝方法，然后在最上层的拷贝方法中去dispatch. 考虑到现在的LLM架构基本类似（gpt decoder-only），k和v的shape基本都是<code>head_dim</code>, <code>head_size</code>, <code>seq_len</code>，<code>batch_size</code>的组合形式，因此我们就直接在写每个modeling的时候将 k.shape = [head_dim, head_k, seq_len], v.shape = [seq_len, head_dim, head_v] 固化下来（both permute好的shape)，这样就可以方便我们只写一种memcpy的方式。假如以后出现了与之不同的情况，也可以通过继承的方式复写对应子类的update函数，保证了一定的灵活性。</p></li><li><p>考虑到会有batch reduction，因此for-loop要注意看管好正在running的prompt的batch idx.，beam_size是不会reduction的，参考上面的对<code>transformers</code>的代码解析。</p></li></ul><p>4). 备选beam的处理，基本上和transformers的逻辑一样，做了一些简化（有些transformers的处理逻辑不是很理解，所以没加），具体c++实现流程参考图和代码（<a target="_blank" rel="noopener" href="https://github.com/intel/neural-speed/blob/v1.0/neural_speed/models/model_utils/model_utils.cpp#L2319-L2325">1</a>, <a target="_blank" rel="noopener" href="https://github.com/intel/neural-speed/blob/v1.0/neural_speed/models/model_utils/model_utils.cpp#L2449-L2506">2</a>）。</p><p>整个的c++的大致流程可以参考这个<a target="_blank" rel="noopener" href="https://github.com/intel/neural-speed/blob/v1.0/neural_speed/models/model_utils/model_utils.cpp#L2508">函数</a>. 也可以用类似于了transformers的方式来验证c++ beam_search的正确性，参考<a target="_blank" rel="noopener" href="https://github.com/intel/neural-speed/blob/v1.0/tests/test_python_api.py#L111-L113">这里</a>.</p><h2 id="continuous-batching">continuous batching</h2><p>细心的朋友可能发现beam_search class除了loop和其包含的成员函数外，还有其他没提到的成员函数，这些函数是为了continuous-batching准备的，具体来说就是将loop拆开成每个step，让外面的调度器来根据实际负载情况单步执行beam_search，执行完时候也会放出完成的request,然后插入新的request. 本质上是多了一些调度处理和first-token和next-token等特殊情况的分叉，核心的东西基本上都是一样的。continuous batching机制的相关代码主要在<a target="_blank" rel="noopener" href="https://github.com/intel/neural-speed/blob/v1.0/neural_speed/models/model_utils/pool.h">pool.h</a>和<a target="_blank" rel="noopener" href="https://github.com/intel/neural-speed/blob/v1.0/neural_speed/models/model_utils/scheduler.h">scheduler.h</a>里，代码逻辑比较简单，也支持了除beam-search外的greedy-search和top_p等sampling的方法。有兴趣的朋友可以自行查阅下。相关资料：<a target="_blank" rel="noopener" href="https://github.com/intel/neural-speed/blob/v1.0/scripts/python_api_example_for_model_server.py">code example</a>, <a target="_blank" rel="noopener" href="https://github.com/intel/neural-speed/blob/v1.0/docs/continuous_batching.md">doc</a></p><h2 id="进一步优化">进一步优化</h2><ul><li><p>prompt-prefix共用一块内存，这个可以用paged kv cache store，也可以对kv cache进行抽象分级存储（system-prompt, initial-tokens, context-tokens, local-contexts等），然后查找重用.</p></li><li><p>paged kv cache存储或许对kv cache的swap比较友好，但是需要重新开发cpu kernel.</p></li></ul></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Richard YU</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://densecollections.top/posts/c++_beam-search-in-neural-speed/">http://densecollections.top/posts/c++_beam-search-in-neural-speed/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://densecollections.top" target="_blank">自拙集</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/LLM/">LLM</a><a class="post-meta__tags" href="/tags/huggingface/">huggingface</a><a class="post-meta__tags" href="/tags/inference/">inference</a></div><div class="post_share"><div class="social-share" data-image="https://plus.unsplash.com/premium_photo-1683121246444-0851419273d8?q=80&amp;w=1780&amp;auto=format&amp;fit=crop&amp;ixlib=rb-4.0.3&amp;ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/posts/Summaryofthisyear-5/"><img class="next-cover" src="https://images.unsplash.com/photo-1708022808984-b8056f0ea8ae?q=80&amp;w=2670&amp;auto=format&amp;fit=crop&amp;ixlib=rb-4.0.3&amp;ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">2023-2024-人生徐徐</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/posts/why-llm-is-memory-bound/" title="浅谈LLM Memory Bound的原因"><img class="cover" src="https://images.unsplash.com/photo-1677442135703-1787eea5ce01?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=2664&q=80" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-09-22</div><div class="title">浅谈LLM Memory Bound的原因</div></div></a></div></div></div><hr><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside_content" id="aside_content"><div class="card-widget card-info"><div class="card-content"><div class="card-info-avatar is-center"><img class="avatar-img" src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/others/avatar.jpg" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"><div class="author-info__name">Richard YU</div><div class="author-info__description">Today everything exists to end in a photograph</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">32</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">62</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">15</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Richardyu114"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://twitter.com/Yu1145635107" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a><a class="social-icon" href="https://instagram.com/d.h.richard" target="_blank" title="Instagram"><i class="fab fa-instagram-square"></i></a><a class="social-icon" href="https://weibo.com/u/5211687990" target="_blank" title="Weibo"><i class="fab fa-weibo"></i></a><a class="social-icon" href="https://www.douban.com/people/161993653/" target="_blank" title="豆瓣"><i class="fas fa-bookmark"></i></a></div></div></div><div class="card-widget card-announcement"><div class="card-content"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">2021-12-10, Wandering and Evolution...</div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="card-content"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#beam-search%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3"><span class="toc-number">1.</span> <span class="toc-text">beam search的基本思想</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#transformers%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="toc-number">2.</span> <span class="toc-text">transformers的实现</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#c-%E5%AE%9E%E7%8E%B0"><span class="toc-number">3.</span> <span class="toc-text">c++实现</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#continuous-batching"><span class="toc-number">4.</span> <span class="toc-text">continuous batching</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%9B%E4%B8%80%E6%AD%A5%E4%BC%98%E5%8C%96"><span class="toc-number">5.</span> <span class="toc-text">进一步优化</span></a></li></ol></div></div></div><div class="card-widget card-recent-post"><div class="card-content"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/posts/c++_beam-search-in-neural-speed/" title="C++复现huggingface transformers中的beam search"><img src="https://plus.unsplash.com/premium_photo-1683121246444-0851419273d8?q=80&amp;w=1780&amp;auto=format&amp;fit=crop&amp;ixlib=rb-4.0.3&amp;ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="C++复现huggingface transformers中的beam search"></a><div class="content"><a class="title" href="/posts/c++_beam-search-in-neural-speed/" title="C++复现huggingface transformers中的beam search">C++复现huggingface transformers中的beam search</a><time datetime="2024-09-10T03:58:56.000Z" title="发表于 2024-09-10 11:58:56">2024-09-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/Summaryofthisyear-5/" title="2023-2024-人生徐徐"><img src="https://images.unsplash.com/photo-1708022808984-b8056f0ea8ae?q=80&amp;w=2670&amp;auto=format&amp;fit=crop&amp;ixlib=rb-4.0.3&amp;ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="2023-2024-人生徐徐"></a><div class="content"><a class="title" href="/posts/Summaryofthisyear-5/" title="2023-2024-人生徐徐">2023-2024-人生徐徐</a><time datetime="2024-02-18T05:59:17.000Z" title="发表于 2024-02-18 13:59:17">2024-02-18</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/why-llm-is-memory-bound/" title="浅谈LLM Memory Bound的原因"><img src="https://images.unsplash.com/photo-1677442135703-1787eea5ce01?ixlib=rb-4.0.3&amp;ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&amp;auto=format&amp;fit=crop&amp;w=2664&amp;q=80" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="浅谈LLM Memory Bound的原因"></a><div class="content"><a class="title" href="/posts/why-llm-is-memory-bound/" title="浅谈LLM Memory Bound的原因">浅谈LLM Memory Bound的原因</a><time datetime="2023-09-22T13:19:07.000Z" title="发表于 2023-09-22 21:19:07">2023-09-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/Summaryofthisyear-4/" title="2022-2023-Suffering"><img src="https://images.unsplash.com/photo-1672092301021-3d810610010f?ixlib=rb-4.0.3&amp;ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&amp;auto=format&amp;fit=crop&amp;w=770&amp;q=80" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="2022-2023-Suffering"></a><div class="content"><a class="title" href="/posts/Summaryofthisyear-4/" title="2022-2023-Suffering">2022-2023-Suffering</a><time datetime="2022-12-29T08:48:19.000Z" title="发表于 2022-12-29 16:48:19">2022-12-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/Summaryofthisyear-3/" title="2021-2022-走在人生的路口"><img src="https://images.unsplash.com/photo-1641928203874-db549ac85d85?ixlib=rb-1.2.1&amp;ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&amp;auto=format&amp;fit=crop&amp;w=387&amp;q=80" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="2021-2022-走在人生的路口"></a><div class="content"><a class="title" href="/posts/Summaryofthisyear-3/" title="2021-2022-走在人生的路口">2021-2022-走在人生的路口</a><time datetime="2022-01-11T13:02:45.000Z" title="发表于 2022-01-11 21:02:45">2022-01-11</time></div></div></div></div></div></div></div></main><footer id="footer" style="background-image:url(https://plus.unsplash.com/premium_photo-1683121246444-0851419273d8?q=80&amp;w=1780&amp;auto=format&amp;fit=crop&amp;ixlib=rb-4.0.3&amp;ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D)"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2024 By Richard YU</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">It's hard to tell that the world we live in is either a reality or a dream.</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div></div><hr><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/search/local-search.js"></script><script>var preloader={endLoading:()=>{document.body.style.overflow="auto",document.getElementById("loading-box").classList.add("loaded")},initLoading:()=>{document.body.style.overflow="",document.getElementById("loading-box").classList.remove("loaded")}};window.addEventListener("load",()=>{preloader.endLoading()})</script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>document.querySelectorAll("#article-container span.katex-display").forEach(a=>{btf.wrap(a,"div","","katex-wrap")})</script><script>function loadValine(){function e(){let e={el:"#vcomment",appId:"duGoywhUmLrx9pM6qQhmf47c-gzGzoHsz",appKey:"m7fc8w4Di5qnAXXaJ5Gp3Pgg",placeholder:"欢迎评论！请留下你的邮箱",avatar:"robohash",meta:"nick,mail,link".split(","),pageSize:"10",lang:"en",recordIP:!1,serverURLs:"",emojiCDN:"",emojiMaps:"",enableQQ:!1,path:window.location.pathname};new Valine(e)}"function"==typeof Valine?e():getScript("https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js").then(e)}{function loadOtherComment(){loadValine()}btf.loadComment(document.querySelector("#vcomment"),loadValine)}</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><div class="app-refresh" id="app-refresh"><div class="app-refresh-wrap"><label>✨ 網站已更新最新版本 👉</label> <a href="javascript:void(0)" onclick="location.reload()">點擊刷新</a></div></div><script>function showNotification(){if(GLOBAL_CONFIG.Snackbar){var t="light"===document.documentElement.getAttribute("data-theme")?GLOBAL_CONFIG.Snackbar.bgLight:GLOBAL_CONFIG.Snackbar.bgDark,e=GLOBAL_CONFIG.Snackbar.position;Snackbar.show({text:"已更新最新版本",backgroundColor:t,duration:5e5,pos:e,actionText:"點擊刷新",actionTextColor:"#fff",onActionClick:function(t){location.reload()}})}else{var o=`top: 0; background: ${"light"===document.documentElement.getAttribute("data-theme")?"#49b1f5":"#1f1f1f"};`;document.getElementById("app-refresh").style.cssText=o}}"serviceWorker"in navigator&&(navigator.serviceWorker.controller&&navigator.serviceWorker.addEventListener("controllerchange",(function(){showNotification()})),window.addEventListener("load",(function(){navigator.serviceWorker.register("/sw.js")})))</script><script>!function(){const t=document.createElement("script"),s=window.location.protocol.split(":")[0];t.src="https"===s?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js",t.dataset.pjax="";const e=document.getElementsByTagName("script")[0];e.parentNode.insertBefore(t,e)}()</script></div></body></html>