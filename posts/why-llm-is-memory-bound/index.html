<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>浅谈LLM Memory Bound的原因 | 自拙集</title><meta name="keywords" content="LLM,Inference"><meta name="author" content="Richard YU"><meta name="copyright" content="Richard YU"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="description" content="现在针对 LLM的部署推理框架都会花很多精力优化IO，公认这种大的decoder-only的模型推理瓶颈在memory侧。对此，我们简单分析一下原因。 首先LLM进行text-generation的流程如下：  加载模型架构 填充weight 输入tokenization化 （tokenizer encode prompt) 计算+后处理 （auto-regression, top_p_top_k"><meta property="og:type" content="article"><meta property="og:title" content="浅谈LLM Memory Bound的原因"><meta property="og:url" content="http://densecollections.top/posts/why-llm-is-memory-bound/index.html"><meta property="og:site_name" content="自拙集"><meta property="og:description" content="现在针对 LLM的部署推理框架都会花很多精力优化IO，公认这种大的decoder-only的模型推理瓶颈在memory侧。对此，我们简单分析一下原因。 首先LLM进行text-generation的流程如下：  加载模型架构 填充weight 输入tokenization化 （tokenizer encode prompt) 计算+后处理 （auto-regression, top_p_top_k"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://images.unsplash.com/photo-1677442135703-1787eea5ce01?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=2664&q=80"><meta property="article:published_time" content="2023-09-22T13:19:07.000Z"><meta property="article:modified_time" content="2023-09-22T13:35:56.775Z"><meta property="article:author" content="Richard YU"><meta property="article:tag" content="LLM"><meta property="article:tag" content="Inference"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://images.unsplash.com/photo-1677442135703-1787eea5ce01?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=2664&q=80"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://densecollections.top/posts/why-llm-is-memory-bound/"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="preconnect" href="//zz.bdstatic.com"><link rel="manifest" href="/image/pwa/manifest.json"><link rel="apple-touch-icon" sizes="180x180" href="/image/pwa/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/image/pwa/32.png"><link rel="icon" type="image/png" sizes="16x16" href="/image/pwa/16.png"><link rel="mask-icon" href="/image/pwa/safari-pinned-tab.svg" color="#5bbad5"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload='this.media="all"'><script>var GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"search.xml",languages:{hits_empty:"找不到您查询的内容：${query}"}},translate:void 0,noticeOutdate:void 0,highlight:{plugin:"highlighjs",highlightCopy:!0,highlightLang:!0},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!1,post:!1},runtime:"",date_suffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:{limitCount:50,languages:{author:"作者: Richard YU",link:"链接: ",source:"来源: 自拙集",info:"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},lightbox:"fancybox",Snackbar:void 0,source:{jQuery:"https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js",justifiedGallery:{js:"https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js",css:"https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css"},fancybox:{js:"https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js",css:"https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"}},isPhotoFigcaption:!0,islazyload:!1,isanchor:!1},saveToLocal={set:function(e,t,s){const n=864e5*s,a={value:t,expiry:(new Date).getTime()+n};localStorage.setItem(e,JSON.stringify(a))},get:function(e){const t=localStorage.getItem(e);if(!t)return;const s=JSON.parse(t);if(!((new Date).getTime()>s.expiry))return s.value;localStorage.removeItem(e)}};const getScript=e=>new Promise((t,s)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=s,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,t())},document.head.appendChild(n)})</script><script id="config_change">var GLOBAL_CONFIG_SITE={isPost:!0,isHome:!1,isHighlightShrink:!0,isToc:!1,postUpdate:"2023-09-22 21:35:56"}</script><noscript><style type="text/css">#nav{opacity:1}.justified-gallery img{opacity:1}#post-meta time,#recent-posts time{display:inline!important}</style></noscript><script>!function(){window.activateDarkMode=function(){document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},window.activateLightMode=function(){document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const e=saveToLocal.get("theme");"dark"===e?activateDarkMode():"light"===e&&activateLightMode();const t=saveToLocal.get("aside-status");void 0!==t&&("hide"===t?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"))}()</script><style type="text/css">.app-refresh{position:fixed;top:-2.2rem;left:0;right:0;z-index:99999;padding:0 1rem;font-size:15px;height:2.2rem;transition:all .3s ease}.app-refresh-wrap{display:flex;color:#fff;height:100%;align-items:center;justify-content:center}.app-refresh-wrap a{color:#fff;text-decoration:underline;cursor:pointer}</style><meta name="generator" content="Hexo 5.4.2"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /><link rel="alternate" href="/atom.xml" title="自拙集" type="application/atom+xml">
</head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/others/avatar.jpg" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">30</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">60</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">14</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> AboutMe</span></a></div><div class="menus_item"><a class="site-page" href="/MindWandering/"><i class="fa-fw fas fa-paper-plane"></i><span> MindWandering</span></a></div><div class="menus_item"><a class="site-page" href="/FunStuff/"><i class="fa-fw fa-solid fa-eye"></i><span> FunStuff</span></a></div><div class="menus_item"><a class="site-page" href="/PaperStation/"><i class="fa-fw fas fa-edit"></i><span> PaperStation</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archive</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div></div></div></div><div id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url(https://images.unsplash.com/photo-1677442135703-1787eea5ce01?ixlib=rb-4.0.3&amp;ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&amp;auto=format&amp;fit=crop&amp;w=2664&amp;q=80)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">自拙集</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> AboutMe</span></a></div><div class="menus_item"><a class="site-page" href="/MindWandering/"><i class="fa-fw fas fa-paper-plane"></i><span> MindWandering</span></a></div><div class="menus_item"><a class="site-page" href="/FunStuff/"><i class="fa-fw fa-solid fa-eye"></i><span> FunStuff</span></a></div><div class="menus_item"><a class="site-page" href="/PaperStation/"><i class="fa-fw fas fa-edit"></i><span> PaperStation</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archive</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">浅谈LLM Memory Bound的原因</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-09-22T13:19:07.000Z" title="发表于 2023-09-22 21:19:07">2023-09-22</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-09-22T13:35:56.775Z" title="更新于 2023-09-22 21:35:56">2023-09-22</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/LLM%E9%83%A8%E7%BD%B2/">LLM部署</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">1.2k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>3分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>现在针对 LLM的部署推理框架都会花很多精力优化IO，公认这种大的decoder-only的模型推理瓶颈在memory侧。对此，我们简单分析一下原因。</p><p>首先LLM进行text-generation的流程如下：</p><ol><li>加载模型架构</li><li>填充weight</li><li>输入tokenization化 （tokenizer encode prompt)</li><li>计算+后处理 （auto-regression, top_p_top_k, beam_search等）</li><li>输出字符（tokenizer decode tokens)</li></ol><p><img src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/why-llm-is-memory-bound/GPT_inference_procedure.png" alt></p><ul><li>⭐架构层面，基本都是transformer decoder搭建的，也就是embedding → decoder_layer (attention_block, norm, residual add, activation op, softmax, …) → lm_head. 其中除了attention_block和lm_head中的GEMM外，其余可以认为都是IO密集型算子 （norm, rope, add, softmax等）。不过现在的推理框架基本都是支持算子 融合（op fusion)，比如 residual add可以作为gemm的post op，reduction维度算完即可相加然后存储，softmax通过flash attention在reduction维度滑动更新， <code>gelu, relu, silu</code> 这种FFN里面的激活函数也可以算出一个block就直接计算等等。总得来说，fusion op来减少反复从memory中反复读写的操作，让核心（core)一次算多点。因此IO OPs方面相对来说比较好解决（profiling看哪个op好事多就想办法fuse起来，压力来到kernel这边）。</li><li>⭐⭐填充weight，也就是 参数量层面。另外模型的层数多，隐藏维度大，导致整体的weight比较大，load起来也比较费时。所以这个是memory bound的第一个原因。但是，现在低精度，混合精度推理相对成熟（ <code>k_quant</code> , <code>weight_only 4 bit</code> 等)，甚至更激进的2 bit也有，因此只要保证精度损失在可接受范围内 （ <code>smooth quant</code> , <code>AWQ</code> , <code>GPTQ</code> 等，压力来到PTQ量化算法这边)，weight就可以压缩好几倍，大幅度减少memory load的时间。</li><li>输入tokenization化，这个是标准操作，每个模型有自己独特的tokenizer进行prompt编码，这个词表查询一般没什么优化技巧，也只在first_token推理中进行</li><li>⭐⭐⭐计算+后处理。这个应该算是最大的IO瓶颈了，大头就是 <code>kv cache</code> ，我们知道LLM都是自回归式（前向注意力）的来计算下一个token，除了prompt的first-token步骤，后面的都必须要串行计算，也就是说下一个token依赖上一个token，以此类推。其中 <code>kv cache</code> 是为了不重复计算之前出现过的token的 <code>K,V</code> activation值进行的优化，也就是**用空间换时间，**防止计算量逐iteration增长。以<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2302.13971.pdf">llama-7b</a> 为例，<code>layer=32, n_hidden_size=4096, context_window=2048</code> , 如果存储fp32 dtype，需要的内存大小为 <code>32x4096x2048x2x4 /1024 /1024=2048M=2GB</code> ，这个还是单batch的，如果想要实现多batch，比如continuous batching，还得乘上max_bs数。第二个是LLM的next_token计算截断，输入的是尺寸是 <code>bsx1</code> ,如果是单batch下，尺寸就非常小，对GEMM来说，计算量小了很多，bottleneck自然就慢慢往IO bound那边倾斜，然后计算阶段还要额外load <code>kv cache</code> 然后进行memory_cpy 最新计算出的 <code>K, V</code> activation值，然后再统一送给下面的 MHA或者flash attention截断。当然，我们可以增大输入的尺寸，比如经典的<a target="_blank" rel="noopener" href="https://www.usenix.org/system/files/osdi22-yu.pdf">ORCA</a> 论文的continuous batching通过把多个request拼成一个sentence来增加计算量，充分利用 <code>core</code> 核心，但是相应的 <code>kv cache</code> 也会成倍增大。因此这一块是最大的瓶颈，没法很好地解决 （tradeoff)，当然 <code>kv cache</code> 也是可以使用低精度来存储，然后计算时候再根据需求反量化回去，比如常见的F16, INT8等，减少IO存储，另一方面比如vllm的 <code>paged attention</code> 来分散其存储位置，减少空间不够的内存搬运等问题。后处理上如果是top_p_top_k这种增加多样性生成的采样方法，应该还好，只是算一次top_k的时间，如果是像 beam_search这种关注句子质量的采样方法，由于要保持多个beam，等于是变向增加了batch_size，因此瓶颈也是在 <code>kv_cache</code> 这里，虽然可以通过common_perfix这种规避掉一些内存，但是可能还要涉及到beam之间的<code>kv_cache</code> 的内存拷贝等，因此memory_bound也很明显。</li><li>输出字符方面和输入类似，只能在tokenizer方面做文章，因为暂时没什么优化，大家都差不多。</li></ul><p>综上，LLM推理的memory_bound最根本的原因还是 <code>kv cache</code> ，而<code>kv cache</code> 是由LLM架构中的前向注意力自回归导致的。如果未来有新的架构可以前向一次prompt吐出多个带前后顺序的tokens，那么<code>kv cache</code> 的问题就能得到很好的缓解了。</p></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Richard YU</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://densecollections.top/posts/why-llm-is-memory-bound/">http://densecollections.top/posts/why-llm-is-memory-bound/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://densecollections.top" target="_blank">自拙集</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/LLM/">LLM</a><a class="post-meta__tags" href="/tags/Inference/">Inference</a></div><div class="post_share"><div class="social-share" data-image="https://images.unsplash.com/photo-1677442135703-1787eea5ce01?ixlib=rb-4.0.3&amp;ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&amp;auto=format&amp;fit=crop&amp;w=2664&amp;q=80" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/posts/Summaryofthisyear-4/"><img class="next-cover" src="https://images.unsplash.com/photo-1672092301021-3d810610010f?ixlib=rb-4.0.3&amp;ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&amp;auto=format&amp;fit=crop&amp;w=770&amp;q=80" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">2022-2023-Suffering</div></div></a></div></nav><hr><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside_content" id="aside_content"><div class="card-widget card-info"><div class="card-content"><div class="card-info-avatar is-center"><img class="avatar-img" src="https://cdn.jsdelivr.net/gh/Richardyu114/PicBed_blog/others/avatar.jpg" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"><div class="author-info__name">Richard YU</div><div class="author-info__description">Today everything exists to end in a photograph</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">30</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">60</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">14</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Richardyu114"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://twitter.com/Yu1145635107" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a><a class="social-icon" href="https://instagram.com/d.h.richard" target="_blank" title="Instagram"><i class="fab fa-instagram-square"></i></a><a class="social-icon" href="https://weibo.com/u/5211687990" target="_blank" title="Weibo"><i class="fab fa-weibo"></i></a><a class="social-icon" href="https://www.douban.com/people/161993653/" target="_blank" title="豆瓣"><i class="fas fa-bookmark"></i></a></div></div></div><div class="card-widget card-announcement"><div class="card-content"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">2021-12-10, Wandering and Evolution...</div></div></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="card-content"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/posts/why-llm-is-memory-bound/" title="浅谈LLM Memory Bound的原因"><img src="https://images.unsplash.com/photo-1677442135703-1787eea5ce01?ixlib=rb-4.0.3&amp;ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&amp;auto=format&amp;fit=crop&amp;w=2664&amp;q=80" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="浅谈LLM Memory Bound的原因"></a><div class="content"><a class="title" href="/posts/why-llm-is-memory-bound/" title="浅谈LLM Memory Bound的原因">浅谈LLM Memory Bound的原因</a><time datetime="2023-09-22T13:19:07.000Z" title="发表于 2023-09-22 21:19:07">2023-09-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/Summaryofthisyear-4/" title="2022-2023-Suffering"><img src="https://images.unsplash.com/photo-1672092301021-3d810610010f?ixlib=rb-4.0.3&amp;ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&amp;auto=format&amp;fit=crop&amp;w=770&amp;q=80" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="2022-2023-Suffering"></a><div class="content"><a class="title" href="/posts/Summaryofthisyear-4/" title="2022-2023-Suffering">2022-2023-Suffering</a><time datetime="2022-12-29T08:48:19.000Z" title="发表于 2022-12-29 16:48:19">2022-12-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/Summaryofthisyear-3/" title="2021-2022-走在人生的路口"><img src="https://images.unsplash.com/photo-1641928203874-db549ac85d85?ixlib=rb-1.2.1&amp;ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&amp;auto=format&amp;fit=crop&amp;w=387&amp;q=80" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="2021-2022-走在人生的路口"></a><div class="content"><a class="title" href="/posts/Summaryofthisyear-3/" title="2021-2022-走在人生的路口">2021-2022-走在人生的路口</a><time datetime="2022-01-11T13:02:45.000Z" title="发表于 2022-01-11 21:02:45">2022-01-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/kk-suggestions-for-young/" title="KK给年轻人的建议"><img src="https://images.unsplash.com/photo-1583382525292-b14a53cb850b?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&amp;ixlib=rb-1.2.1&amp;auto=format&amp;fit=crop&amp;w=1868&amp;q=80" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="KK给年轻人的建议"></a><div class="content"><a class="title" href="/posts/kk-suggestions-for-young/" title="KK给年轻人的建议">KK给年轻人的建议</a><time datetime="2021-05-10T12:15:18.000Z" title="发表于 2021-05-10 20:15:18">2021-05-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/Summaryofthisyear-2/" title="2020-2021-人间喜乐"><img src="https://images.unsplash.com/photo-1543087369-5efd51802307?ixid=MXwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHw%3D&amp;ixlib=rb-1.2.1&amp;auto=format&amp;fit=crop&amp;w=2000&amp;q=80" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="2020-2021-人间喜乐"></a><div class="content"><a class="title" href="/posts/Summaryofthisyear-2/" title="2020-2021-人间喜乐">2020-2021-人间喜乐</a><time datetime="2021-01-17T07:55:36.000Z" title="发表于 2021-01-17 15:55:36">2021-01-17</time></div></div></div></div></div></div></div></main><footer id="footer" style="background-image:url(https://images.unsplash.com/photo-1677442135703-1787eea5ce01?ixlib=rb-4.0.3&amp;ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&amp;auto=format&amp;fit=crop&amp;w=2664&amp;q=80)"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2023 By Richard YU</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">It's hard to tell that the world we live in is either a reality or a dream.</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div></div><hr><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/search/local-search.js"></script><script>var preloader={endLoading:()=>{document.body.style.overflow="auto",document.getElementById("loading-box").classList.add("loaded")},initLoading:()=>{document.body.style.overflow="",document.getElementById("loading-box").classList.remove("loaded")}};window.addEventListener("load",()=>{preloader.endLoading()})</script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>document.querySelectorAll("#article-container span.katex-display").forEach(a=>{btf.wrap(a,"div","","katex-wrap")})</script><script>function loadValine(){function e(){let e={el:"#vcomment",appId:"duGoywhUmLrx9pM6qQhmf47c-gzGzoHsz",appKey:"m7fc8w4Di5qnAXXaJ5Gp3Pgg",placeholder:"欢迎评论！请留下你的邮箱",avatar:"robohash",meta:"nick,mail,link".split(","),pageSize:"10",lang:"en",recordIP:!1,serverURLs:"",emojiCDN:"",emojiMaps:"",enableQQ:!1,path:window.location.pathname};new Valine(e)}"function"==typeof Valine?e():getScript("https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js").then(e)}{function loadOtherComment(){loadValine()}btf.loadComment(document.querySelector("#vcomment"),loadValine)}</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><div class="app-refresh" id="app-refresh"><div class="app-refresh-wrap"><label>✨ 網站已更新最新版本 👉</label> <a href="javascript:void(0)" onclick="location.reload()">點擊刷新</a></div></div><script>function showNotification(){if(GLOBAL_CONFIG.Snackbar){var t="light"===document.documentElement.getAttribute("data-theme")?GLOBAL_CONFIG.Snackbar.bgLight:GLOBAL_CONFIG.Snackbar.bgDark,e=GLOBAL_CONFIG.Snackbar.position;Snackbar.show({text:"已更新最新版本",backgroundColor:t,duration:5e5,pos:e,actionText:"點擊刷新",actionTextColor:"#fff",onActionClick:function(t){location.reload()}})}else{var o=`top: 0; background: ${"light"===document.documentElement.getAttribute("data-theme")?"#49b1f5":"#1f1f1f"};`;document.getElementById("app-refresh").style.cssText=o}}"serviceWorker"in navigator&&(navigator.serviceWorker.controller&&navigator.serviceWorker.addEventListener("controllerchange",(function(){showNotification()})),window.addEventListener("load",(function(){navigator.serviceWorker.register("/sw.js")})))</script><script>!function(){const t=document.createElement("script"),s=window.location.protocol.split(":")[0];t.src="https"===s?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js",t.dataset.pjax="";const e=document.getElementsByTagName("script")[0];e.parentNode.insertBefore(t,e)}()</script></div></body></html>