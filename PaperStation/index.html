<!DOCTYPE html>












  


<html class="theme-next gemini use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">




  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2">







  <meta name="google-site-verification" content="true">







  <meta name="baidu-site-verification" content="true">



  
  
  <link rel="stylesheet" href="/lib/fancybox/source/jquery.fancybox.css">







<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2">

<link rel="stylesheet" href="/css/main.css?v=7.1.1">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.1.1">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.1.1">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.1.1">


  <link rel="mask-icon" href="/images/logo.svg?v=7.1.1" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.1.1',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: false,
    fancybox: true,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  






  <meta name="baidu-site-verification" content="0bqk4mbBLD">



  <meta name="google-site-verification" content="FvMbHVgnH7FO0GFRzTmOevjSaFwvzIQZv94LdufMMPY">


  <meta name="description" content="About记录自己看到的与计算机视觉，视觉SLAM，机器人，机器学习等相关的论文。如果是比较重要的和自己感兴趣的论文会另开一篇post详细介绍。 2019.8Focal Loss paper code  loss的具体形式为：$criterion= \alpha(1-a)^{\gamma}y \ln a + (1-\alpha)a^{\gamma}(1-y) \ln (1-a)$，主要的作用就是提">
<meta property="og:type" content="website">
<meta property="og:title" content="PaperStation">
<meta property="og:url" content="http://richardyu114.github.io/PaperStation/index.html">
<meta property="og:site_name" content="自拙集">
<meta property="og:description" content="About记录自己看到的与计算机视觉，视觉SLAM，机器人，机器学习等相关的论文。如果是比较重要的和自己感兴趣的论文会另开一篇post详细介绍。 2019.8Focal Loss paper code  loss的具体形式为：$criterion= \alpha(1-a)^{\gamma}y \ln a + (1-\alpha)a^{\gamma}(1-y) \ln (1-a)$，主要的作用就是提">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://raw.githubusercontent.com/Richardyu114/copies-of-posts-of-my-personal-blog/master/images/model_Thoracic%20Disease%20Identification%20and%20Localization%20with%20Limited%20Supervision.PNG">
<meta property="og:image" content="https://raw.githubusercontent.com/Richardyu114/copies-of-posts-of-my-personal-blog/master/images/model_FCN.PNG">
<meta property="og:updated_time" content="2019-08-22T08:34:19.410Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="PaperStation">
<meta name="twitter:description" content="About记录自己看到的与计算机视觉，视觉SLAM，机器人，机器学习等相关的论文。如果是比较重要的和自己感兴趣的论文会另开一篇post详细介绍。 2019.8Focal Loss paper code  loss的具体形式为：$criterion= \alpha(1-a)^{\gamma}y \ln a + (1-\alpha)a^{\gamma}(1-y) \ln (1-a)$，主要的作用就是提">
<meta name="twitter:image" content="https://raw.githubusercontent.com/Richardyu114/copies-of-posts-of-my-personal-blog/master/images/model_Thoracic%20Disease%20Identification%20and%20Localization%20with%20Limited%20Supervision.PNG">



  <link rel="alternate" href="/atom.xml" title="自拙集" type="application/atom+xml">



  
  
  <link rel="canonical" href="http://richardyu114.github.io/PaperStation/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>PaperStation | 自拙集</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">自拙集</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">Work cures everything</p>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">

    
    
    
      
    

    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>关于</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-paperstation menu-item-active">

    
    
    
      
    

    

    <a href="/PaperStation/" rel="section"><i class="menu-item-icon fa fa-fw fa-edit"></i> <br>PaperStation</a>

  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>搜索</a>
        </li>
      
    </ul>
  

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

    
    
      
      
    
      
      
    
      
      
    
      
      
    
      
      
    
      
      
    
    

  


          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    
    
    
    <div class="post-block page">
      <header class="post-header">

<h1 class="post-title" itemprop="name headline">PaperStation

</h1>

<div class="post-meta">
  
  



</div>

</header>

      
      
      
      <div class="post-body">
        
        
          <h2 id="About"><a href="#About" class="headerlink" title="About"></a>About</h2><p>记录自己看到的与计算机视觉，视觉SLAM，机器人，机器学习等相关的论文。如果是比较重要的和自己感兴趣的论文会另开一篇post详细介绍。</p>
<h2 id="2019-8"><a href="#2019-8" class="headerlink" title="2019.8"></a>2019.8</h2><h3 id="Focal-Loss"><a href="#Focal-Loss" class="headerlink" title="Focal Loss"></a>Focal Loss</h3><ul>
<li><a href="https://arxiv.org/abs/1708.02002" target="_blank" rel="noopener">paper</a></li>
<li><a href="https://github.com/facebookresearch/Detectron" target="_blank" rel="noopener">code</a></li>
</ul>
<p>loss的具体形式为：$criterion= \alpha(1-a)^{\gamma}y \ln a + (1-\alpha)a^{\gamma}(1-y) \ln (1-a)$，主要的作用就是提高对假阴性的惩罚力度，在<a href="https://arxiv.org/abs/1708.02002" target="_blank" rel="noopener">论文</a>中作者指出，对于设计的RetinaNet，超参数$\alpha=2, \gamma=0.25$效果最好（分类目标检测阶段的前景和背景分离），在我实际的二分类使用中，效果并不是十分突出，参数的调节是个技术活，否则很容易使得假阳性很高，不过这可能也是和数据集有关。</p>
<p>pytorch代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">#自定义的模型和loss都要继承nn.Module类</span><br><span class="line">class BFocalLoss(nn.Module):</span><br><span class="line">      def __init__(self, gamma=2, alpha=0.25):</span><br><span class="line">          super(BFocalLoss, self).__init__()</span><br><span class="line">          self.gamma = gamma</span><br><span class="line">          self.alpha = alpha</span><br><span class="line">      def forward(self, inputs, targets):</span><br><span class="line">          pt = nn.Softmax(input, dim=1)</span><br><span class="line">          p = pt[:,1]</span><br><span class="line">          loss = -self.alpha * (1-p) ** self.gamma*(target*torch.log(p+1e-12)) - \</span><br><span class="line">                 (1-self.alpha)*p**self.gamma*((1-target)*torch.log(1-p+1e-12))</span><br><span class="line">          return loss.mean()</span><br></pre></td></tr></table></figure>
<p>medium上一篇<a href="https://towardsdatascience.com/retinanet-how-focal-loss-fixes-single-shot-detection-cb320e3bb0de" target="_blank" rel="noopener">blog</a>对flocal loss进行了阐释。</p>
<h3 id="Deep-Learning-in-Tumor-Metastatic-on-Medicine-Image"><a href="#Deep-Learning-in-Tumor-Metastatic-on-Medicine-Image" class="headerlink" title="Deep Learning in Tumor Metastatic on Medicine Image"></a>Deep Learning in Tumor Metastatic on Medicine Image</h3><p>两篇深度学习在乳腺癌细胞转移检测的论文：</p>
<ul>
<li><a href="https://arxiv.org/pdf/1606.05718.pdf" target="_blank" rel="noopener">Deep Learning for Identifying Metastatic Breast Cancer</a></li>
<li><a href="https://arxiv.org/pdf/1703.02442.pdf" target="_blank" rel="noopener">Detecting Cancer Metastases on Gigapixel Pathology Images</a></li>
</ul>
<p>医学图像处理与自然图像处理不同，一般来说由于设备的原因，可能病灶特征不是特别容易区分，也不是很明显，因此ImageNet上的预训练模型可能不是很有用。医学图像方面由于图像数量少，标注成本高，所以用的tricks比较多，要根据具体的要求和数据采集情况分析，比如痰涂片载玻片图像，一般得到的数据集可能是组与组之间是连续的特征，就像视频中连续帧的图像，差别不会很大，因此标注的时候可能只需要根据采集的组进行少量标注就可以，进行弱监督训练，也可能达到很不错的分类精度。</p>
<ul>
<li>特定的数据增强，RGB-HSV转换，color normalization</li>
<li>slide选取patches放大不同尺度，多尺度输入</li>
<li>原始样本旋转90，180，270度，left-right flip之后再旋转90，180，270度，这样就扩增到了8倍大小。然后进行图像色调的调整，包括对比度，亮度，饱和度等。</li>
<li>FROC 而不是ROC和AUC（performance衡量标准）</li>
<li>减少计算，移除背景patches</li>
<li>随机森林提取heatmap特征</li>
</ul>
<h3 id="Thoracic-Disease-Identification-and-Localization-with-Limited-Supervision"><a href="#Thoracic-Disease-Identification-and-Localization-with-Limited-Supervision" class="headerlink" title="Thoracic Disease Identification and Localization with Limited Supervision"></a>Thoracic Disease Identification and Localization with Limited Supervision</h3><p><a href="https://arxiv.org/pdf/1711.06373v6.pdf" target="_blank" rel="noopener">paper</a></p>
<p><a href="https://github.com/romanovar/evaluation_MIL" target="_blank" rel="noopener">code</a></p>
<p>这是平安科技和李飞飞合作的论文，主要是通过图片级label标注和少量框标注来达到病灶regions显示的目的，生成heatmap和类别判定。</p>
<blockquote>
<p>Given images with disease labels and limited bounding box information, we aim to design a unified model that simultaneously produces disease identification and localization.</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/Richardyu114/copies-of-posts-of-my-personal-blog/master/images/model_Thoracic%20Disease%20Identification%20and%20Localization%20with%20Limited%20Supervision.PNG" alt="model_overview"></p>
<p>大致的网络结构是利用resnet backbone进行下采样，生成$P \times P$的feature map，然后对应原图的空间尺寸，有框的话就进行像素级的比对预测，没框但是有图片级的label的话那么这张图片中肯定存在至少一个patch对应于这种疾病，就直接根据概率进行预测。（多实例学习）</p>
<ul>
<li>利用resnet结构，去掉global pooling和fc层，利用卷积block下采样图片，得到feature map</li>
<li>不同size的input图片最后得到的feature map尺寸不同，为了得到设定的patch大小，大的feature map进行maxpooling进行下采样，小的利用bilinear interpolation（双线性插值）扩大</li>
<li>feature map送入全卷积网络，先通过$3 \times 3$的卷积层，卷积核大小$c^{*}=512$，然后在送入$1 \times 1$的卷积层，生成$P \times P \times K$的feature map，也就是$K$个类别的预测图，根据每个图上预测的概率值进行判断属于哪个类别。（output map与target map进行loss定位回归）</li>
</ul>
<p>损失函数就是利用权重$\lambda$调节框监督和无框标注的重要程度，利用二元参数$\eta$来选择图片的监督信息对应的loss函数。</p>
<p>对于每个patch在之前的过程中都预测出了对应的score，在定位阶段设置一个阈值，只要score大于该值即可认为它是positive patch，论文中的阈值设置为0.5。之前论文中提到，并不预测出严格的位置来，而是一个大概区域。因为经过阈值判定的结果并不会精确地分布在一个规则的矩形框之中。</p>
<p>个人觉得这篇文章的两个亮点在于：</p>
<ul>
<li>医疗影像处理，论文实验做的也不少</li>
<li>不同的监督信息混合，多实例学习，得到区域分割和类别判断的效果</li>
</ul>
<p>但是通篇读下来觉得收获并不是很大，废话比较多，让人耳目一新的东西并不多，如果不是Feifei Li挂名的话，可能登不上CVPR。不过实习的项目想拿这篇文章的思想来做些东西，于是便选择读了。</p>
<h3 id="FCN-Fully-Convolutional-Networks-for-Semantic-Segmentation"><a href="#FCN-Fully-Convolutional-Networks-for-Semantic-Segmentation" class="headerlink" title="FCN: Fully Convolutional Networks for Semantic Segmentation"></a>FCN: Fully Convolutional Networks for Semantic Segmentation</h3><p><a href="https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf" target="_blank" rel="noopener">paper</a></p>
<p><a href="https://github.com/pytorch/vision/tree/master/torchvision/models/segmentation" target="_blank" rel="noopener">code</a></p>
<p>开始之前先了解下上采样中的反卷积(deconvoltion)方法（一般有双线性插值，反卷积和反池化三种方法）。</p>
<p>实际上，转置卷积(transposed convolution)这种叫法可能更为合适。因为反卷积的数学含义，通过反卷积可以将通过卷积的输出信号，完全还原输入信号，而事实是，转置卷积只能还原shape大小，不能还原value. <a href="https://zhuanlan.zhihu.com/p/48501100" target="_blank" rel="noopener">知乎专栏</a>, <a href="https://www.zhihu.com/question/43609045" target="_blank" rel="noopener">知乎</a></p>
<blockquote>
<p>转置卷积是一种特殊的正向卷积，先按照一定的比例通过补0来扩大输入图像的尺寸，接着旋转卷积核，再进行正向卷积。</p>
<p>输入元素矩阵$X$，输出元素矩阵$Y$，正向卷积的矩阵为$C$，则正常的卷积操作过程为：$Y = CX$，反卷积的操作就是要对这个矩阵运算过程进行逆运算，即: $X=C^{T}Y$。但是此操作只会恢复矩阵大小，并不会恢复元素值。</p>
<p>转置卷积的公式：</p>
<script type="math/tex; mode=display">
\begin{align}
& o = \text{size of output} \\
&i = \text{size of input} \\
&p = \text{padding} \\
&s = \text{strides}
\end{align}</script><p>对于正常卷积：$o=[(i+2p-k)/s]+1$</p>
<p>对于转置卷积：</p>
<p>如果$(o+2p-k) \% s =0$，则可以通过$0 = s(i-1)-2p+k $来确定卷积核参数，</p>
<p>如果$(o+2p-k) \% s \not =0$，则可以通过$o = s(i-1)-2p+k+(0+2p-k)\% s $来确定卷积核参数</p>
</blockquote>
<p>FCN是深度学习+语义分割领域的的milestone论文。语义分割的基本思想和分类相同，只不过是预测图像中的每个像素的所属类别（对类别进行数字编码，加上无关背景，总共是$n_{classes}+1$类，然后每个类别可以用不同的颜色显示），然后计算像素级别的loss，再进行back propagation。FCN也是这种训练方式，最后生成的特征图的个数是类别个数，所有特征图的每个像素进行log_softmax预测属于哪种类别，最后解码成设定的RGB图像。</p>
<p><strong>architecture+implementation</strong>:</p>
<p><img src="https://raw.githubusercontent.com/Richardyu114/copies-of-posts-of-my-personal-blog/master/images/model_FCN.PNG" alt="architecture of FCN"></p>
<p>原文对backbone的选择，以及训练集选择，数据预处理，训练数据采样，训练过程中的超参等进行了实验，因此实验内容还是十分丰富的，最终表现好的backbone 是VGG-16，然后实验之后认为利用卷积层提取特征，pooling层下采样5次后预测的精度比较高，结构中亮点在于skip connections，采样较少的feature map尺寸大，保留了比较多的local信息，对小物体表式比较好，采样较多的feature map尺寸小，特征层级高，体现的是global信息，然而对小物体表示比较差，因此lower layer和higher layer融合可以比较好的全面表示图像的语义信息，这个结构提出了这个思考，同时也进行了实验，分别给出不融合，融合一层，融合两层的结构，看各自的表现如何。实验也表明FCN-8s的精度最好，而且越融合精度提升的点不大，但是会使得最后的结果更加smooth。</p>
<p>训练过程中权重初始化是个很重要的trick，论文是先利用在ImageNet上预训练过的模型去掉最后的全连接层来初始化权重，然后进行后面三个pooling过的feature map对应融合再去最后的上采样（full卷积，即反卷积）得到$c\times h \times w$的图像。FCN中的full卷积方式是先对特征图进行padding，用0填充，然后再进行正常的卷积操作，得到上采样的结果，反卷积层的权重初始化也是个很重要的部份，论文中说使用随机初始化效果不怎么样，这里可以使用<code>bilinear kernel</code>，此函数在pytorch中也有，下面的代码讲解中也有具体介绍。对于最后的loss计算，图像的每个像素点有21（针对pascal VOC数据集，最后网络输出的是$c\times h \times w$个特征图，$c=21$）个归一化概率值，最大的索引代表类别数字，也就是说是每个像素会进行一个loss计算，每个图像有$h\times w$个像素值，最会全部加起来进行backpropagation，等于是图片级分类问题中的一个”batch”操作。</p>
<p><a href="https://zhuanlan.zhihu.com/p/32506912" target="_blank" rel="noopener">pytorch实现FCN代码讲解</a></p>
<p><a href="https://www.cnblogs.com/gujianhan/p/6030639.html" target="_blank" rel="noopener">blog详解FCN网络</a></p>
<p><strong>metric</strong>：</p>
<p>假设$n_{ij}$是将类别$i$的像素预测成类别$j$的像素的个数，$n_{cl}$是物体的类别数，$t_{i}=\sum _{j}n_{ij}$是预测成像素$i$的总个数（包含正确预测的和把其他类别预测成$i$的）</p>
<ul>
<li><p>pixel accuracy: $ \sum_{i} n_{i i} / \sum_{i} t_{i}$</p>
</li>
<li><p>mean accuracy: $\left(1 / n_{\mathrm{cl}}\right) \sum_{i} n_{i i} / t_{i}$</p>
</li>
<li><p>mean IU(region intersection over union): $\left(1 / n_{\mathrm{cl}}\right) \sum_{i} n_{i i} /\left(t_{i}+\sum_{j} n_{j i}-n_{i i}\right)$</p>
</li>
<li><p>frequency weighted IU: $\left(\sum_{k} t_{k}\right)^{-1} \sum_{i} t_{i} n_{i i} /\left(t_{i}+\sum_{j} n_{j i}-n_{i i}\right)$</p>
</li>
</ul>
<p>FCN在语义分割（semantic segmentation）和场景理解（scene parsing）领域中是一项非常重要的工作，模型思想很简单，同时也是非常具有逻辑性的，但是论文读起来个人感觉不是特别顺畅，可能是自己的原因，觉得讲得比较乱，可能是因为作者做了很多的实验性尝试去试着提高精度，但是都没什么太大的用处，但是又不得不写出来突出工作量的缘故。抛开论文不谈，FCN的代码工作是不简单的，尤其是相对图像级别的分类任务而言，有很多流程和细节部份需要注意，否则训练可能会得不到好的效果，比如数据读取数据预处理部份（图像级label-RGB值编码-类别数字编码对应抽取对应，图像对应crop等），以及权重初始化，卷积核设计等，自己完整写一遍FCN的代码并且调试出好的结果肯定会大有裨益。</p>
<h3 id="U-Net-Convolutional-Networks-for-Biomedical-Image-Segmentation"><a href="#U-Net-Convolutional-Networks-for-Biomedical-Image-Segmentation" class="headerlink" title="U-Net: Convolutional Networks for Biomedical Image Segmentation"></a>U-Net: Convolutional Networks for Biomedical Image Segmentation</h3><p><a href="https://arxiv.org/pdf/1505.04597.pdf" target="_blank" rel="noopener">paper</a></p>
<p><a href="https://github.com/LeeJunHyun/Image_Segmentation" target="_blank" rel="noopener">code1</a></p>
<p><a href="https://github.com/milesial/Pytorch-UNet" target="_blank" rel="noopener">code2</a></p>
<p><a href="https://github.com/ShawnBIT/UNet-family" target="_blank" rel="noopener">U-Net family</a></p>
<p>U-Net是医学图像分割领域效果很好的一个网络架构，对称的下采样和上采样操作以及skip connections保证了特征图既包含了低层级的特征，也包含了高层级的语义特征，适合单一和图像梯度复杂的医学图像，实际在自然图像中表现也不错。（<a href="https://www.zhihu.com/question/269914775" target="_blank" rel="noopener">知乎：为什么U-Net在医学图像分割领域表现不错</a>）</p>
<h3 id="U-Net-A-Nested-U-Net-Architecture-for-Medical-Image-Segmentation"><a href="#U-Net-A-Nested-U-Net-Architecture-for-Medical-Image-Segmentation" class="headerlink" title="U-Net++: A Nested U-Net Architecture for Medical Image Segmentation"></a>U-Net++: A Nested U-Net Architecture for Medical Image Segmentation</h3><p><a href="https://arxiv.org/abs/1807.10165" target="_blank" rel="noopener">paper</a></p>
<p><a href="https://github.com/MrGiovanni/UNetPlusPlus" target="_blank" rel="noopener">code</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/44958351" target="_blank" rel="noopener">blog</a></p>
<h3 id="BoxSup-Exploiting-Bounding-Boxes-to-Supervise-Convolutional-Networks-for-Semantic-Segmentation"><a href="#BoxSup-Exploiting-Bounding-Boxes-to-Supervise-Convolutional-Networks-for-Semantic-Segmentation" class="headerlink" title="BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation"></a>BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation</h3><p><a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Dai_BoxSup_Exploiting_Bounding_ICCV_2015_paper.pdf" target="_blank" rel="noopener">paper</a></p>
<h3 id="Simple-Does-It-Weakly-Supervised-Instance-and-Semantic-Segmentation"><a href="#Simple-Does-It-Weakly-Supervised-Instance-and-Semantic-Segmentation" class="headerlink" title="Simple Does It: Weakly Supervised Instance and Semantic Segmentation"></a>Simple Does It: Weakly Supervised Instance and Semantic Segmentation</h3><p><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Khoreva_Simple_Does_It_CVPR_2017_paper.pdf" target="_blank" rel="noopener">paper</a></p>
<h3 id="Seed-Expand-and-Constrain-Three-Principles-for-Weakly-Supervised-Image-Segmentation"><a href="#Seed-Expand-and-Constrain-Three-Principles-for-Weakly-Supervised-Image-Segmentation" class="headerlink" title="Seed, Expand and Constrain: Three Principles for Weakly-Supervised Image Segmentation"></a>Seed, Expand and Constrain: Three Principles for Weakly-Supervised Image Segmentation</h3><p><a href="https://arxiv.org/pdf/1603.06098.pdf" target="_blank" rel="noopener">paper</a></p>
<h3 id="Mixup"><a href="#Mixup" class="headerlink" title="Mixup"></a>Mixup</h3><ul>
<li><a href="https://arxiv.org/abs/1710.09412" target="_blank" rel="noopener">paper</a></li>
<li><a href="https://github.com/facebookresearch/mixup-cifar10" target="_blank" rel="noopener">code</a></li>
</ul>
<p>adversarial examples， ERM（经验风险最小化）准则不能很好的适用，数据增强，VRM（近邻风险最小化），插值生成对抗样本和标签</p>
<script type="math/tex; mode=display">
\widetilde x   = \lambda x_{i} + (1-\lambda)x_{j}\\
\widetilde y  = \lambda y_{i} + (1-\lambda)y_{j} \\</script><p>对交叉熵损失函数和Focal Loss而言，可以直接取出$y_{i}, y_{j}$对其损失函数进行插值(数学上可以推导)</p>
<script type="math/tex; mode=display">
loss = \lambda \cdot criterion(\widetilde x, y_{i}) + (1-\lambda) \cdot criterion(\widetilde x, y_{j})</script><blockquote>
<p>The mixup vicinal distribution can be understood as a form of data augmentation that encourages the model f to behave linearly in-between training examples. We argue that this linear behaviour reduces the amount of undesirable oscillations when predicting outside the training examples. Also, linearity is a good inductive bias from the perspective of Occam’s razor, since it is one of the simplest possible behaviors.</p>
</blockquote>
<p>论文指出，mixup可以控制模型复杂度，也就是说模型在ERM情况下不断训练会记住training data，导致泛化能力差，而mixup通过随机pairing插值融合，生成对抗样本可以有效的缓解这种情况。而且通过大量的实验，证明mixup确实有效果，而且在各个领域都还不错，此外可以和dropout等控制模型复杂度方法相结合。</p>
<blockquote>
<p>We have shown that mixup is a form of vicinal risk minimization, which trains on virtual examples<br>constructed as the linear interpolation of two random examples from the training set and their labels. Incorporating mixup into existing training pipelines reduces to a few lines of code, and introduces little or no computational overhead. Throughout an extensive evaluation, we have shown that mixup improves the generalization error of state-of-the-art models on ImageNet, CIFAR, speech, and tabular datasets. Furthermore, mixup helps to combat memorization of corrupt labels, sensitivity to adversarial examples, and instability in adversarial training.</p>
</blockquote>
<p>在图像上的训练trick: 训练时每个epoch都采用mixup，$\lambda$的取值由$Beta(\alpha,\alpha)$函数随机指定，$\alpha$是hyper-parameter，论文中指出在imagNet上的值在[0.1, 0.4]之间，在CIFAR上取的是1，此外，网络结构加深和训练周期的加长都会使得最终的泛化效果比较好。但是论文中没有将为什么选用$beta$函数去生成$\lambda$，优化器选的是带动量的SGD，其中learning rate会随着指定的epoch范围进行下降，且没有使用drop out。</p>
<p>numpy.random.beta()是对beta分布进行随机采样，下式是beta分布的概率密度函数，当$\alpha$的取值越大时，取样的值基本就会往0.5靠近，这时候似乎就退化成sample pairing。</p>
<script type="math/tex; mode=display">
\lambda = f(x ; a, b)=\frac{1}{B(\alpha, \beta)} x^{\alpha-1}(1-x)^{\beta-1} \\
B(\alpha, \beta)=\int_{0}^{1} t^{\alpha-1}(1-t)^{\beta-1} dt</script><p>mixup与IBM的一篇文章<a href="https://arxiv.org/abs/1801.02929" target="_blank" rel="noopener">sample pairing</a>的想法很类似，而且提出的时间都差不多，不过sample pairing是随机将两幅图片平均插值，但是label不变，等于是引入噪声，而且训练的trick也比较多，可以参考这篇<a href="https://jsideas.net/samplepairing/" target="_blank" rel="noopener">博客</a>的实验。</p>
<p>如果加入warmup，学习率随指定epoch下降，weight decay=$10^{-4}$（对于mixup，小的weight decay效果 更好)，此外超参$\alpha$的取值越大，训练集的loss会越大，但是泛化能力就会越好。但是具体的数据集可能training loss变化趋势不同，可能随着$\alpha$的增加急剧增加，也可能不怎么变化，因此最佳的位置，作者也提出了疑问，放在了discussion中，他们猜测可能大容量模型可能会对大取值$\alpha$的适应度好点。</p>
<blockquote>
<p>In our experiments, the following trend is consistent: with increasingly large $\alpha$, the training error on<br>real data increases, while the generalization gap decreases.</p>
</blockquote>
<h3 id="Detecting-Lesion-Bounding-Ellipses-With-Gaussian-Proposal-Networks-GPN"><a href="#Detecting-Lesion-Bounding-Ellipses-With-Gaussian-Proposal-Networks-GPN" class="headerlink" title="Detecting Lesion Bounding Ellipses With Gaussian Proposal Networks(GPN)"></a>Detecting Lesion Bounding Ellipses With Gaussian Proposal Networks(GPN)</h3><p>椭圆标注的目标检测，比如医学图像中deep-lesion，人脸检测。</p>
<p>一种直觉的方法是通过三个点（中心点，长轴点，短轴点）和一个旋转角度$\tan\theta$，仿照faster r-cnn来进行回归，但是长短轴比例ration较大或者接近与1的时候，就会要求角度预测的很准确或者不需要准确，而且two-stage效率比较低。</p>
<p>GPN源于二维高斯分布，通过ground truth的概率分布图的等高线（椭圆）来和预测的做距离上的regression，这样就大大降低了参数量，对于旋转的情况，利用rotation matrix来进行坐标系的平移（关键词：协方差矩阵，KLD损失函数，概率密度函数，像素密度<foreground,backgroud>)</foreground,backgroud></p>
<h3 id="CAM—Learning-Deep-Features-for-Discriminative-Localization"><a href="#CAM—Learning-Deep-Features-for-Discriminative-Localization" class="headerlink" title="CAM—Learning Deep Features for Discriminative Localization"></a>CAM—Learning Deep Features for Discriminative Localization</h3><ul>
<li><a href="https://arxiv.org/pdf/1512.04150.pdf" target="_blank" rel="noopener">paper</a></li>
<li>[code]</li>
</ul>
<p>卷积单元即使在没有监督下也可以detect object，但是这种能力会在后面的全连接层下消失，通过global average pooling可以保持这种能力。</p>
<p><em>class activation mapping</em></p>
<h3 id="Visual-SLAM几篇综述"><a href="#Visual-SLAM几篇综述" class="headerlink" title="Visual SLAM几篇综述"></a>Visual SLAM几篇综述</h3><ul>
<li><a href="http://www.cad.zju.edu.cn/home/gfzhang/projects/JCAD2016-SLAM-survey.pdf" target="_blank" rel="noopener">基于单目视觉的同时定位与地图构建方法综述</a></li>
</ul>
<blockquote>
<p>摘 要: 增强现实是一种在现实场景中无缝地融入虚拟物体或信息的技术, 能够比传统的文字、图像和视频等方式更高效、直观地呈现信息，有着非常广泛的应用. 同时定位与地图构建作为增强现实的关键基础技术, 可以用来在未知环境中定位自身方位并同时构建环境三维地图, 从而保证叠加的虚拟物体与现实场景在几何上的一致性. 文中首先简述基于视觉的同时定位与地图构建的基本原理; 然后介绍几个代表性的基于单目视觉的同时定位与地图构建方法并做深入分析和比较; 最后讨论近年来研究热点和发展趋势, 并做总结和展望。</p>
</blockquote>
<p>中文综述目前看的比较舒服的一篇，对于visual SLAM有一定了解的人看起来很快，同时也梳理得比较整洁紧凑。</p>
<ul>
<li><a href="https://www.researchgate.net/profile/Jose_Ascencio/publication/234081012_Visual_Simultaneous_Localization_and_Mapping_A_Survey/links/55383e610cf247b8587d3d58/Visual-Simultaneous-Localization-and-Mapping-A-Survey.pdf" target="_blank" rel="noopener">Visual simultaneous localization and mapping: a survey</a></li>
</ul>
<blockquote>
<p>Abstract : Visual SLAM (simultaneous localization and mapping) refers to the problem of using images, as the only source of external information, in order to establish the position of a robot, a vehicle, or a moving camera in an environment, and at the same time, construct a representation of the explored zone. SLAM is an essential task for the autonomy of a robot. Nowadays, the problem of SLAM is considered solved when range sensors such as lasers or sonar are used to built 2D maps of small static environments. However SLAM for dynamic, complex and large scale environments, using vision as the sole external sensor, is an active area of research. The computer vision techniques employed in visual SLAM, such as detection, description and matching of salient features, image recognition and retrieval, among others, are still susceptible of improvement. The objective of this article is to provide new researchers in the field of visual SLAM a brief and comprehensible review of the state-of-the-art.</p>
</blockquote>
<p>这是一篇非常棒的综述，对于入门的人非常友好，几乎没有数学公式，只是对slam中的各个问题和模块进行了分解，词汇也不复杂，读起来很快。但是缺点是不是很新，而且深度不够。</p>
<ul>
<li><a href="https://arxiv.org/pdf/1606.05830.pdf" target="_blank" rel="noopener">Past, Present, and Future of Simultaneous Localization And Mapping: Towards the Robust-Perception Age</a></li>
</ul>
<blockquote>
<p>Abstract—Simultaneous Localization and Mapping (SLAM) consists in the concurrent construction of a model of the environment (the map), and the estimation of the state of the robot moving within it. The SLAM community has made astonishing progress over the last 30 years, enabling large-scale real-world applications, and witnessing a steady transition of this technology to industry. We survey the current state of SLAM. We start by presenting what is now the de-facto standard formulation for SLAM. We then review related work, covering a broad set of topics including robustness and scalability in long-term mapping, metric and semantic representations for mapping, theoretical performance guarantees, active SLAM and exploration, and other new frontiers. This paper simultaneously serves as a position paper and tutorial to those who are users of SLAM. By looking at the published research with a critical eye, we delineate open challenges and new research issues, that still deserve careful scientific investigation. The paper also contains the authors’ take on two questions that often animate discussions during robotics conferences: Do robots need SLAM? and Is SLAM solved?</p>
</blockquote>
<p>这个综述相对难说难度点，但是写的非常好，毕竟作者都是有名的大佬。比较难得的是，这篇综述不仅梳理了slam的发展历程和技术现状，还提出了一些”open problem”，表明了自己的的观点，详细地阐述了视觉SLAM现在的挑战以及未来可能的应对办法，虽然有些问题是显而易见的。此外，该综述中也提供了很多参考文献，尤其是对于场景识别中的感知混叠以及滤波器优化和非线性优化的比较，以及因子图的功效，后续都值得研究一下。</p>
<ul>
<li><a href="https://arxiv.org/abs/1803.11288" target="_blank" rel="noopener">FutureMapping: The Computational Structure of Spatial AI Systems,Andrew J. Davison</a></li>
</ul>
<blockquote>
<p>We discuss and predict the evolution of Simultaneous Localisation and Mapping (SLAM) into a general geometric and semantic ‘Spatial AI’ perception capability for intelligent embodied devices. A big gap remains between the visual perception performance that devices such as augmented reality eyewear or consumer robots will require and what is possible within the constraints imposed by real products. Co-design of algorithms, processors and sensors will be needed. We explore the computational structure of current and future Spatial AI algorithms and consider this within the landscape of ongoing hardware developments.</p>
</blockquote>
<p><a href="https://richardyu114.github.io/2019/04/16/FutureMapping-by-A-J-Davison/">blog</a></p>
<ul>
<li><a href="https://www.researchgate.net/profile/Ruihao_Li4/publication/327531951_Ongoing_Evolution_of_Visual_SLAM_from_Geometry_to_Deep_Learning_Challenges_and_Opportunities/links/5c8650db92851c1d5e156d7f/Ongoing-Evolution-of-Visual-SLAM-from-Geometry-to-Deep-Learning-Challenges-and-Opportunities.pdf" target="_blank" rel="noopener">Ongoing Evolution of Visual SLAM from Geometry to Deep Learning: Challenges and Opportunities</a></li>
</ul>
<p>这篇综述主要关注的是深度学习在slam中的应用，先介绍了几种常见的模型，即CNN, RNN和encoder, decoder，然后列举了一些slam常用的dataset，包括KITTI, TUM,  NYU等，接着分块介绍深度学习在depth estimation, pose estimation, ego-motion estimation, relocalization, sensor fusion, semantic mapping方面的应用概况，总的来说在位姿估计，深度尺度估计，回环检测重定位，地图构建这几个方面着手，论文最后提出了一些存在的挑战和思路，总体来说介绍的还是挺全的，列举的文章也很经典。但是总觉得深度不够，像是一种在知乎上回答问题的方式。虽然值得看，不过等到以后发现了更好的综述再来替换吧。</p>
<h3 id="Deep-Learning-Visual-Odometry"><a href="#Deep-Learning-Visual-Odometry" class="headerlink" title="Deep Learning + Visual Odometry"></a>Deep Learning + Visual Odometry</h3><ul>
<li>[deepvo]</li>
<li>[code]</li>
</ul>
<ul>
<li>[sfmlearner]</li>
<li>[code]</li>
</ul>
<ul>
<li>[undeepvo]</li>
<li>[code]</li>
</ul>
<ul>
<li><a href="https://arxiv.org/pdf/1809.05786.pdf" target="_blank" rel="noopener">GANvo</a></li>
<li>[code]</li>
</ul>
<ul>
<li>[SGANvo](</li>
</ul>

        
      </div>
      
      
      
    </div>
    



    
    
    
  </div>


          </div>
          

  
    <div class="comments" id="comments">
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/uploads/header.jpg" alt="Richard YU">
            
              <p class="site-author-name" itemprop="name">Richard YU</p>
              <div class="site-description motion-element" itemprop="description">Today everything exists to end in a photograph</div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">10</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                      <a href="/categories/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">6</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                      <a href="/tags/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">31</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://github.com/richardyu114" title="GitHub &rarr; https://github.com/richardyu114" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://twitter.com/Yu1145635107" title="Twitter &rarr; https://twitter.com/Yu1145635107" rel="noopener" target="_blank"><i class="fa fa-fw fa-twitter"></i>Twitter</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://instagram.com/d.h.richard" title="Instagram &rarr; https://instagram.com/d.h.richard" rel="noopener" target="_blank"><i class="fa fa-fw fa-instagram"></i>Instagram</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://weibo.com/u/5211687990" title="Weibo &rarr; https://weibo.com/u/5211687990" rel="noopener" target="_blank"><i class="fa fa-fw fa-weibo"></i>Weibo</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://www.douban.com/people/161993653/" title="豆瓣 &rarr; https://www.douban.com/people/161993653/" rel="noopener" target="_blank"><i class="fa fa-fw fa-paperclip"></i>豆瓣</a>
                </span>
              
            </div>
          

          
             <div class="cc-license motion-element" itemprop="license">
              
              
                
              
              
              
              <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
             </div>
          

          
          

          
            
          
          

        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#About"><span class="nav-number">1.</span> <span class="nav-text">About</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2019-8"><span class="nav-number">2.</span> <span class="nav-text">2019.8</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Focal-Loss"><span class="nav-number">2.1.</span> <span class="nav-text">Focal Loss</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Deep-Learning-in-Tumor-Metastatic-on-Medicine-Image"><span class="nav-number">2.2.</span> <span class="nav-text">Deep Learning in Tumor Metastatic on Medicine Image</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Thoracic-Disease-Identification-and-Localization-with-Limited-Supervision"><span class="nav-number">2.3.</span> <span class="nav-text">Thoracic Disease Identification and Localization with Limited Supervision</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#FCN-Fully-Convolutional-Networks-for-Semantic-Segmentation"><span class="nav-number">2.4.</span> <span class="nav-text">FCN: Fully Convolutional Networks for Semantic Segmentation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#U-Net-Convolutional-Networks-for-Biomedical-Image-Segmentation"><span class="nav-number">2.5.</span> <span class="nav-text">U-Net: Convolutional Networks for Biomedical Image Segmentation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#U-Net-A-Nested-U-Net-Architecture-for-Medical-Image-Segmentation"><span class="nav-number">2.6.</span> <span class="nav-text">U-Net++: A Nested U-Net Architecture for Medical Image Segmentation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#BoxSup-Exploiting-Bounding-Boxes-to-Supervise-Convolutional-Networks-for-Semantic-Segmentation"><span class="nav-number">2.7.</span> <span class="nav-text">BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Simple-Does-It-Weakly-Supervised-Instance-and-Semantic-Segmentation"><span class="nav-number">2.8.</span> <span class="nav-text">Simple Does It: Weakly Supervised Instance and Semantic Segmentation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Seed-Expand-and-Constrain-Three-Principles-for-Weakly-Supervised-Image-Segmentation"><span class="nav-number">2.9.</span> <span class="nav-text">Seed, Expand and Constrain: Three Principles for Weakly-Supervised Image Segmentation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Mixup"><span class="nav-number">2.10.</span> <span class="nav-text">Mixup</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Detecting-Lesion-Bounding-Ellipses-With-Gaussian-Proposal-Networks-GPN"><span class="nav-number">2.11.</span> <span class="nav-text">Detecting Lesion Bounding Ellipses With Gaussian Proposal Networks(GPN)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CAM—Learning-Deep-Features-for-Discriminative-Localization"><span class="nav-number">2.12.</span> <span class="nav-text">CAM—Learning Deep Features for Discriminative Localization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Visual-SLAM几篇综述"><span class="nav-number">2.13.</span> <span class="nav-text">Visual SLAM几篇综述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Deep-Learning-Visual-Odometry"><span class="nav-number">2.14.</span> <span class="nav-text">Deep Learning + Visual Odometry</span></a></li></ol></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Richard YU</span>

  

  
</div>


  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.8.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.1.1</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>












  















  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script src="/lib/fancybox/source/jquery.fancybox.pack.js"></script>


  


  <script src="/js/utils.js?v=7.1.1"></script>

  <script src="/js/motion.js?v=7.1.1"></script>



  
  


  <script src="/js/affix.js?v=7.1.1"></script>

  <script src="/js/schemes/pisces.js?v=7.1.1"></script>




  
  <script src="/js/scrollspy.js?v=7.1.1"></script>
<script src="/js/post-details.js?v=7.1.1"></script>



  


  <script src="/js/next-boot.js?v=7.1.1"></script>


  

  

  

  
  

<script src="//cdn1.lncld.net/static/js/3.11.1/av-min.js"></script>



<script src="//unpkg.com/valine/dist/Valine.min.js"></script>

<script>
  var GUEST = ['nick', 'mail', 'link'];
  var guest = 'nick,mail,link';
  guest = guest.split(',').filter(function(item) {
    return GUEST.indexOf(item) > -1;
  });
  new Valine({
    el: '#comments',
    verify: true,
    notify: true,
    appId: '0dPFtfvMW3unG026ng9mltSq-gzGzoHsz',
    appKey: 'PMHx38PGqlVax1npmbxpY2ig',
    placeholder: '欢迎评论！请留下你的邮箱',
    avatar: 'mm',
    meta: guest,
    pageSize: '10' || 10,
    visitor: true,
    lang: '' || 'zh-cn'
  });
</script>




  


  
  <script>
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      
        // ref: https://github.com/ForbesLindesay/unescape-html
        var unescapeHtml = function(html) {
          return String(html)
            .replace(/&quot;/g, '"')
            .replace(/&#39;/g, '\'')
            .replace(/&#x3A;/g, ':')
            // replace all the other &#x; chars
            .replace(/&#(\d+);/g, function (m, p) { return String.fromCharCode(p); })
            .replace(/&lt;/g, '<')
            .replace(/&gt;/g, '>')
            .replace(/&amp;/g, '&');
        };
      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                content = unescapeHtml(content);
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url).replace(/\/{2,}/g, '/');
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x"></i></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x"></i></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
  

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });
  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') { next = next.nextSibling }
        if (next && next.nodeName.toLowerCase() === 'br') { next.parentNode.removeChild(next) }
      }
    });
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      document.getElementById(all[i].inputID + '-Frame').parentNode.className += ' has-jax';
    }
  });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    
  


  

  

  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>


  

  

  

  

  

  
<script>
  $('.highlight').not('.gist .highlight').each(function(i, e) {
    var $wrap = $('<div>').addClass('highlight-wrap');
    $(e).after($wrap);
    $wrap.append($('<button>').addClass('copy-btn').append('复制').on('click', function(e) {
      var code = $(this).parent().find('.code').find('.line').map(function(i, e) {
        return $(e).text();
      }).toArray().join('\n');
      var ta = document.createElement('textarea');
      var yPosition = window.pageYOffset || document.documentElement.scrollTop;
      ta.style.top = yPosition + 'px'; // Prevent page scroll
      ta.style.position = 'absolute';
      ta.style.opacity = '0';
      ta.readOnly = true;
      ta.value = code;
      document.body.appendChild(ta);
      ta.select();
      ta.setSelectionRange(0, code.length);
      ta.readOnly = false;
      var result = document.execCommand('copy');
      
        if (result) $(this).text('复制成功');
        else $(this).text('复制失败');
      
      ta.blur(); // For iOS
      $(this).blur();
    })).on('mouseleave', function(e) {
      var $b = $(this).find('.copy-btn');
      setTimeout(function() {
        $b.text('复制');
      }, 300);
    }).append(e);
  })
</script>


  

  

</body>
</html>
